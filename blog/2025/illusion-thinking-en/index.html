<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity | Yu-Cheng Chang </title> <meta name="author" content="Yu-Cheng Chang"> <meta name="description" content="This Apple paper reveals a shocking truth: reasoning models aren't always better with more complexity! Research shows that when problems exceed a certain complexity threshold, performance actually declines even with more thinking time. This challenges our basic understanding of AI reasoning capabilities."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://codepydog.github.io/blog/2025/illusion-thinking-en/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yu-Cheng</span> Chang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</h1> <p class="post-meta"> Created on June 07, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/reasoning"> <i class="fa-solid fa-hashtag fa-sm"></i> reasoning</a>   <a href="/blog/tag/llm"> <i class="fa-solid fa-hashtag fa-sm"></i> llm</a>   <a href="/blog/tag/evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> evaluation</a>   ·   <a href="/blog/category/paper"> <i class="fa-solid fa-tag fa-sm"></i> paper</a>   <a href="/blog/category/english"> <i class="fa-solid fa-tag fa-sm"></i> english</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <div class="language-switcher" style="text-align: right; margin-bottom: 20px; padding: 8px 0; border-bottom: 1px solid #333;"> <span style="font-size: 14px; color: #888; margin-right: 8px;">🌐 Language:</span> <a href="/blog/2025/illusion-thinking-chs/" style="color: #007bff; text-decoration: none; font-weight: 500; transition: opacity 0.2s;" onmouseover="this.style.opacity='0.7'" onmouseout="this.style.opacity='1'">中文</a> <span style="color: #666; margin: 0 6px;">|</span> <strong style="color: #007bff; font-weight: 600;">English</strong> </div> <blockquote> <p><strong>Source:</strong> <a href="https://arxiv.org/pdf/2506.06941" rel="external nofollow noopener" target="_blank">Paper</a></p> </blockquote> <p>This is a groundbreaking paper from Apple, officially titled <strong><a href="https://arxiv.org/pdf/2506.06941" rel="external nofollow noopener" target="_blank">The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</a></strong>. This paper challenges our fundamental assumptions about the capabilities of large reasoning models and poses a thought-provoking question: Are current reasoning models truly “thinking,” or are they merely executing complex pattern matching?</p> <h2 id="core-problem-what-are-the-true-capabilities-of-reasoning-models">Core Problem: What Are the True Capabilities of Reasoning Models?</h2> <h3 id="limitations-of-current-evaluation-methods">Limitations of Current Evaluation Methods</h3> <p>Current evaluations of large language models’ reasoning capabilities primarily suffer from the following issues:</p> <blockquote> <p><em>“Current evaluations primarily focus on established mathematical and coding benchmarks, emphasizing final answer accuracy. However, these approaches fail to provide insights into the reasoning traces’ structure and quality.”</em></p> </blockquote> <p><strong>Three Core Problems:</strong></p> <ol> <li> <strong>Over-emphasis on Final Answer Accuracy</strong>: Current evaluations mainly focus on whether results are correct, while ignoring the quality of the reasoning process</li> <li> <strong>Lack of Deep Analysis of Reasoning Structure</strong>: No systematic analysis of models’ internal reasoning traces</li> <li> <strong>Insufficient Understanding of Problem Complexity</strong>: Lack of framework for understanding model performance from a problem complexity perspective</li> </ol> <h3 id="the-rise-and-challenges-of-large-reasoning-models">The Rise and Challenges of Large Reasoning Models</h3> <p>The paper identifies a key trend:</p> <blockquote> <p><em>“Recent generations of frontier language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood.”</em></p> </blockquote> <p>Although new-generation reasoning models (such as Claude 3.7 Thinking, GPT-o1, etc.) perform excellently on benchmarks, our understanding of their true capabilities remains insufficient.</p> <h2 id="innovative-solution-systematic-analysis-framework-based-on-complexity">Innovative Solution: Systematic Analysis Framework Based on Complexity</h2> <h3 id="core-methodology">Core Methodology</h3> <p>Apple’s research team proposed a revolutionary evaluation framework:</p> <blockquote> <p><em>“In this work, we systematically investigate these aspects of LRMs by constructing puzzle environments that allow precise manipulation of computational complexity while maintaining consistent logical structures.”</em></p> </blockquote> <p><strong>Three Key Innovations:</strong></p> <ol> <li> <p><strong>Controllable Puzzle Environment Design</strong></p> <ul> <li>Precise control of computational complexity</li> <li>Maintaining consistent logical structures</li> <li>Eliminating external variable interference</li> </ul> </li> <li> <p><strong>Reasoning Trace Analysis</strong></p> <ul> <li>Simultaneous analysis of final answers and intermediate reasoning processes</li> <li>Complete reasoning path verification from initial state to target state</li> <li>Multi-dimensional performance evaluation</li> </ul> </li> <li> <p><strong>Three Key Performance Metrics</strong></p> <ul> <li><strong>Final Answer Accuracy</strong></li> <li><strong>Reasoning Trace Quality</strong></li> <li><strong>Computational Efficiency</strong></li> </ul> </li> </ol> <h3 id="ingenious-experimental-design">Ingenious Experimental Design</h3> <p>The research team designed a controllable complexity puzzle environment, with the advantage being:</p> <blockquote> <p><em>“This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into LRMs’ computational behavior.”</em></p> </blockquote> <p>By adjusting puzzle size and step count, researchers can precisely control the computational complexity of problems while maintaining consistency in logical structure.</p> <h2 id="shocking-discovery-counterintuitive-complexity-scaling-boundaries">Shocking Discovery: Counterintuitive Complexity Scaling Boundaries</h2> <h3 id="core-finding">Core Finding</h3> <p>The paper’s most important discovery challenges a fundamental assumption in the AI field:</p> <blockquote> <p><em>“Moreover, they exhibit a counterintuitive scaling limit: their reasoning effect increases with problem complexity up to a point, then declines despite having an adequate token budget.”</em></p> </blockquote> <p><strong>Key Insights:</strong></p> <ul> <li>Reasoning effectiveness improves with problem complexity, but only up to a critical point</li> <li>Beyond the critical point, performance still declines even with adequate token budget</li> <li>This phenomenon reveals fundamental limitations of current reasoning models</li> </ul> <h3 id="experimental-results-analysis">Experimental Results Analysis</h3> <p>From the paper’s experimental results, we can see three important patterns:</p> <ol> <li> <p><strong>Accuracy vs Complexity Curve</strong>:</p> <ul> <li>As complexity increases, model accuracy shows an inverted U-shaped curve</li> <li>There exists an optimal complexity point, beyond which performance drops sharply</li> </ul> </li> <li> <p><strong>Token Usage Patterns</strong>:</p> <ul> <li>Models dynamically adjust thinking length based on problem complexity</li> <li>But beyond a certain point, increasing thinking length cannot improve performance</li> </ul> </li> <li> <p><strong>Reasoning Quality Differences</strong>:</p> <ul> <li>In correctly solved cases, models tend to find answers early</li> <li>In failed cases, models often focus on wrong directions, wasting computational resources</li> </ul> </li> </ol> <h2 id="deep-thinking-the-nature-of-thinking">Deep Thinking: The Nature of “Thinking”</h2> <h3 id="the-paradox-of-reasoning-efficiency">The Paradox of Reasoning Efficiency</h3> <p>The paper discovered an interesting phenomenon:</p> <blockquote> <p><em>“Both cases reveal inefficiencies in the reasoning process.”</em></p> </blockquote> <p><strong>Two Types of Inefficient Patterns:</strong></p> <ol> <li> <strong>Premature Convergence</strong>: May overthink on simple problems</li> <li> <strong>Error Fixation</strong>: Easily trapped in wrong thinking on complex problems and difficult to self-correct</li> </ol> <h3 id="implications-for-agi-development">Implications for AGI Development</h3> <p>The paper poses a profound philosophical question:</p> <blockquote> <p><em>“emergence suggests a potential paradigm shift in how LLM systems approach complex reasoning and problem-solving tasks, with some researchers proposing them as significant steps toward more general artificial intelligence capabilities.”</em></p> </blockquote> <p>But also maintains a rational attitude:</p> <blockquote> <p><em>“Despite these claims and performance advancements, the fundamental benefits and limitations of LRMs remain insufficiently understood.”</em></p> </blockquote> <h2 id="practical-value-and-future-directions">Practical Value and Future Directions</h2> <h3 id="guidance-for-practical-applications">Guidance for Practical Applications</h3> <p>This research provides important practical guidance for AI applications:</p> <p><strong>1. Cost Optimization Strategies</strong></p> <ul> <li>Help determine optimal reasoning resource allocation</li> <li>Avoid wasting resources on tasks beyond the optimal complexity point</li> </ul> <p><strong>2. Performance Expectation Management</strong></p> <ul> <li>Set reasonable performance expectations for tasks of different complexity</li> <li>Understand the boundaries of model capabilities</li> </ul> <p><strong>3. Model Selection Guidelines</strong></p> <ul> <li>Select appropriate reasoning models for specific applications</li> <li>Balance performance and cost</li> </ul> <h3 id="future-research-directions">Future Research Directions</h3> <p>This work opens several important research directions:</p> <p><strong>1. Adaptive Reasoning</strong></p> <ul> <li>How to make models dynamically adjust reasoning strategies based on problem complexity</li> <li>Develop complexity-aware reasoning algorithms</li> </ul> <p><strong>2. Reasoning Efficiency Optimization</strong></p> <ul> <li>How to improve reasoning efficiency while maintaining accuracy</li> <li>Design smarter computational resource allocation mechanisms</li> </ul> <p><strong>3. Evaluation Methodology Innovation</strong></p> <ul> <li>Develop more comprehensive reasoning capability evaluation frameworks</li> <li>Focus on process rather than just results</li> </ul> <h2 id="personal-reflections-and-insights">Personal Reflections and Insights</h2> <h3 id="similarities-with-human-cognition">Similarities with Human Cognition</h3> <p>This research reminds me of human cognitive patterns when solving complex problems. Humans also experience similar “efficiency boundaries”:</p> <ul> <li> <strong>The Trap of Overthinking</strong>: Sometimes thinking too much actually reduces problem-solving effectiveness</li> <li> <strong>Cognitive Load Limitations</strong>: Thinking quality declines when cognitive capacity is exceeded</li> <li> <strong>Intuition vs Analysis</strong>: Simple problems rely on intuition, complex problems need systematic analysis</li> </ul> <p>Does this similarity suggest that current reasoning models have indeed captured some characteristics of cognitive processes to some extent?</p> <h3 id="echoing-thinking-fast-and-slow-theory">Echoing “Thinking, Fast and Slow” Theory</h3> <p>This research seems to provide empirical support for Daniel Kahneman’s “Thinking, Fast and Slow” theory in AI applications:</p> <ul> <li> <strong>System 1 (Fast Thinking)</strong>: Suitable for handling simple, familiar problems</li> <li> <strong>System 2 (Slow Thinking)</strong>: Suitable for handling complex problems requiring deep analysis</li> </ul> <p>The performance differences of models on problems of varying complexity echo the dual-system theory of human cognition.</p> <h3 id="considerations-for-ai-safety">Considerations for AI Safety</h3> <p>This research also reminds us to pay attention to the reliability of AI systems:</p> <ul> <li> <strong>Importance of Capability Boundaries</strong>: Understanding model limitations is crucial for safe deployment</li> <li> <strong>Risk of Overconfidence</strong>: Models may show inappropriate confidence on problems beyond their capability range</li> <li> <strong>Necessity of Explainability</strong>: Need better understanding of models’ reasoning processes</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>This paper provides a completely new perspective for understanding large reasoning models. By revealing the counterintuitive phenomenon of “reasoning effectiveness complexity boundaries,” it challenges our basic assumptions about AI capabilities and reminds us that when pursuing more powerful AI systems, we need to understand these systems’ working principles more deeply.</p> <p><strong>Core Insights:</strong></p> <ul> <li>More computational resources don’t always lead to better performance</li> <li>Reasoning models have fundamental capability boundaries</li> <li>We need to rethink how to evaluate and optimize reasoning capabilities</li> </ul> <p>This work not only provides important insights for current AI research but also points the way for building more reliable and efficient reasoning systems in the future. As the paper’s title suggests, we need to see through the “illusion of thinking” to understand the true nature of reasoning models.</p> <hr> <h2 id="references">References</h2> <ul> <li><a href="https://arxiv.org/pdf/2506.06941" rel="external nofollow noopener" target="_blank">The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</a></li> <li><a href="https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow" rel="external nofollow noopener" target="_blank">Thinking, Fast and Slow - Daniel Kahneman</a></li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/3min-paper-illusion-thinking/">[3min-Paper] The_Illusion_of_Thinking</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/illusion-illusion-thinking-en/">The Illusion of the Illusion of Thinking: When AI Evaluation Methods Become Traps for Capability Assessment</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/illusion-illusion-thinking-chs/">[中文版] The Illusion of the Illusion of Thinking: 當AI評估方法成為能力判斷的陷阱</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yu-Cheng Chang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>