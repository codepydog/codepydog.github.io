<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> The Illusion of the Illusion of Thinking: When AI Evaluation Methods Become Traps for Capability Assessment | Yu-Cheng Chang </title> <meta name="author" content="Yu-Cheng Chang"> <meta name="description" content="This commentary paper reveals a shocking truth: we often mistake the limitations of AI evaluation methods for the limitations of AI systems themselves! Research shows that many cases considered AI reasoning failures are actually misjudgments caused by poorly designed evaluation frameworks."> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="data:image/svg+xml,&lt;svg%20xmlns=%22http://www.w3.org/2000/svg%22%20viewBox=%220%200%20100%20100%22&gt;&lt;text%20y=%22.9em%22%20font-size=%2290%22&gt;%E2%9A%9B%EF%B8%8F&lt;/text&gt;&lt;/svg&gt;"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://codepydog.github.io/blog/2025/illusion-illusion-thinking-en/"> <script src="/assets/js/theme.js?a81d82887dd692e91686b43de4542f18"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Yu-Cheng</span> Chang </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/projects/">projects </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item "> <a class="nav-link" href="/teaching/">teaching </a> </li> <li class="nav-item "> <a class="nav-link" href="/people/">people </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/books/">bookshelf</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">The Illusion of the Illusion of Thinking: When AI Evaluation Methods Become Traps for Capability Assessment</h1> <p class="post-meta"> Created on June 16, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/ai-evaluation"> <i class="fa-solid fa-hashtag fa-sm"></i> ai-evaluation</a>   <a href="/blog/tag/reasoning"> <i class="fa-solid fa-hashtag fa-sm"></i> reasoning</a>   <a href="/blog/tag/model-limitations"> <i class="fa-solid fa-hashtag fa-sm"></i> model-limitations</a>   <a href="/blog/tag/methodology"> <i class="fa-solid fa-hashtag fa-sm"></i> methodology</a>   ·   <a href="/blog/category/paper"> <i class="fa-solid fa-tag fa-sm"></i> paper</a>   <a href="/blog/category/english"> <i class="fa-solid fa-tag fa-sm"></i> english</a> </p> </header> <article class="post-content"> <div id="markdown-content"> <div class="language-switcher" style="text-align: right; margin-bottom: 20px; padding: 8px 0; border-bottom: 1px solid #333;"> <span style="font-size: 14px; color: #888; margin-right: 8px;">🌐 Language:</span> <a href="/blog/2025/illusion-illusion-thinking-chs/" style="color: #007bff; text-decoration: none; font-weight: 500; transition: opacity 0.2s;" onmouseover="this.style.opacity='0.7'" onmouseout="this.style.opacity='1'">中文</a> <span style="color: #666; margin: 0 6px;">|</span> <strong style="color: #007bff; font-weight: 600;">English</strong> </div> <blockquote> <p><strong>Source:</strong> <a href="https://arxiv.org/pdf/2506.09250" rel="external nofollow noopener" target="_blank">Paper</a></p> </blockquote> <p>This is an important commentary paper by A. Lawson, officially titled <strong><a href="https://arxiv.org/pdf/2506.09250" rel="external nofollow noopener" target="_blank">The Illusion of the Illusion of Thinking: A Comment on Shojaee et al. (2025)</a></strong>. This paper responds to Shojaee et al. (2025)’s research on Large Reasoning Model limitations, which builds upon the Apple paper we previously analyzed. If you’re interested in this background, you can refer to our previous notes: <a href="/blog/2025/illusion-thinking-en/">The Illusion of Thinking: Apple Paper Analysis</a>.</p> <p>This commentary presents a profound perspective: <strong>we often mistake the limitations of evaluation methods for the limitations of AI systems themselves</strong>.</p> <h2 id="core-problem-the-trap-of-evaluation-methods">Core Problem: The Trap of Evaluation Methods</h2> <p>Shojaee et al. (2025) claimed to have discovered fundamental limitations of Large Reasoning Models through systematic evaluation of planning puzzles, finding that model accuracy “collapses” to zero beyond certain complexity thresholds.</p> <blockquote> <p><em>“Shojaee et al. (2025) claim to have identified fundamental limitations in Large Reasoning Models through systematic evaluation on planning puzzles. Their central finding—that model accuracy ‘collapses’ to zero beyond certain complexity thresholds—has significant implications for AI reasoning research.”</em></p> </blockquote> <p>However, this commentary paper points out that these apparent failures actually stem from experimental design limitations rather than fundamental reasoning capability defects of the models themselves.</p> <blockquote> <p><em>“However, our analysis reveals that these apparent failures stem from experimental design limitations rather than fundamental reasoning failures.”</em></p> </blockquote> <h2 id="three-key-problems">Three Key Problems</h2> <h3 id="problem-one-models-actively-recognize-output-constraints">Problem One: Models Actively Recognize Output Constraints</h3> <p><strong>Most Important Finding:</strong> Models can actually actively recognize when they approach output limits and make reasonable stopping decisions.</p> <p>The paper cites a replication experiment by @scaling01 on Twitter, which captured model outputs explicitly stating:</p> <blockquote> <p><em>“The pattern continues, but to avoid making this too long, I’ll stop here”</em></p> </blockquote> <p>This proves that models understand the solution pattern but choose to truncate output due to practical constraints.</p> <blockquote> <p><em>“This demonstrates that models understand the solution pattern but choose to truncate output due to practical constraints.”</em></p> </blockquote> <p><strong>Key Insight:</strong> Misjudging this model behavior as “reasoning collapse” reflects a fundamental problem with automated evaluation systems that fail to account for model awareness and decision-making capabilities.</p> <h3 id="problem-two-mathematical-analysis-of-token-limitations">Problem Two: Mathematical Analysis of Token Limitations</h3> <p>The paper provides detailed mathematical analysis explaining why Tower of Hanoi problems encounter token limitations:</p> <p><strong>Linear Growth Pattern (outputting only final sequence):</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>T_final(N) ≈ 10(2^N - 1) + C
</code></pre></div></div> <p><strong>Maximum Solvable Scale:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>N_max ≈ log₂(L_max/10)
</code></pre></div></div> <p>For different token limits:</p> <ul> <li>L_max = 64,000: N_max ≈ 12-13</li> <li>L_max = 100,000: N_max ≈ 13</li> </ul> <p><strong>Key Finding:</strong> The reported “collapse” occurs before N = 9, well before these theoretical limits.</p> <blockquote> <p><em>“Interestingly, the reported ‘collapse’ before N = 9 for most models occurs well before these theoretical limits. This suggests that models are making a decision to terminate output before actually reaching their context window limits.”</em></p> </blockquote> <h3 id="problem-three-the-trap-of-impossible-puzzles">Problem Three: The Trap of Impossible Puzzles</h3> <p>In River Crossing experiments, evaluation issues are dramatically compounded:</p> <blockquote> <p><em>“The evaluation issues are compounded dramatically in the River Crossing experiments. Shojaee et al. test instances with N ≥ 6 actors/agents using boat capacity b = 3. However, it is a well-established result that the Missionaries-Cannibals puzzle (and its variants) has no solution for N &gt; 5 when b = 3.”</em></p> </blockquote> <p><strong>Problem Essence:</strong> The evaluation framework includes mathematically impossible problems, then attributes models’ inability to solve these problems to insufficient reasoning capabilities.</p> <h2 id="experimental-validation-alternative-representations-restore-performance">Experimental Validation: Alternative Representations Restore Performance</h2> <p>To test whether failures reflect reasoning limitations or format constraints, the author conducted preliminary testing:</p> <p><strong>Test Prompt:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"Solve Tower of Hanoi with 15 disks. Output a Lua function that prints the solution when called."
</code></pre></div></div> <p><strong>Surprising Results:</strong></p> <ul> <li>Multiple advanced models (Claude-3.7-Sonnet, Claude Opus 4, OpenAI o3, Google Gemini 2.5) achieved very high accuracy</li> <li>Completed within 5,000 tokens</li> <li>Generated solutions correctly implemented recursive algorithms</li> </ul> <blockquote> <p><em>“The generated solutions correctly implement the recursive algorithm, demonstrating intact reasoning capabilities when freed from exhaustive enumeration requirements.”</em></p> </blockquote> <h2 id="reevaluating-complexity-claims">Reevaluating Complexity Claims</h2> <p>The paper points out problems with the original research using “compositional depth” (minimum moves) as a complexity metric:</p> <table> <thead> <tr> <th>Puzzle</th> <th>Solution Length</th> <th>Branching Factor</th> <th>Computational Complexity</th> </tr> </thead> <tbody> <tr> <td>Tower of Hanoi</td> <td>2^N - 1</td> <td>1</td> <td>O(1) per move</td> </tr> <tr> <td>Blocks World</td> <td>O(N)</td> <td>O(N²)</td> <td>Linear (near-optimal) / NP-hard (optimal)</td> </tr> </tbody> </table> <p><strong>Key Insight:</strong> Tower of Hanoi, despite requiring exponentially many moves, has a trivial O(1) decision process per move. Blocks World is much harder.</p> <h3 id="the-optimality-question">The Optimality Question</h3> <p>Blocks World prompts explicitly require optimization:</p> <blockquote> <p><em>“Find the minimum sequence of moves to transform the initial state into the goal state.”</em></p> </blockquote> <p>While the solution checker only verifies correctness rather than optimality, models attempt to follow instructions to find optimal solutions, increasing computational burden.</p> <h2 id="deep-thinking-the-importance-of-evaluation-design">Deep Thinking: The Importance of Evaluation Design</h2> <h3 id="engineering-decisions-vs-reasoning-failures">Engineering Decisions vs Reasoning Failures</h3> <p>The paper’s most innovative insight is distinguishing between “engineering decisions” and “reasoning failures”:</p> <blockquote> <p><em>“This distinction further emphasizes the importance of evaluation design. Scoring models as ‘failures’ for making reasonable engineering decisions about output length mischaracterizes their actual capabilities.”</em></p> </blockquote> <p><strong>Core Viewpoint:</strong> When models choose not to output lengthy intermediate steps, this may be reasonable efficiency consideration, not capability defect.</p> <h3 id="models-self-awareness-capabilities">Models’ Self-Awareness Capabilities</h3> <p>Research found models possess some form of self-calibration ability:</p> <blockquote> <p><em>“This behavior indicates that models may be poorly calibrated about their own context length capabilities, choosing to stop prematurely.”</em></p> </blockquote> <p><strong>Important Finding:</strong> Models demonstrate some form of self-awareness about their capabilities, making stopping decisions based on circumstances.</p> <h2 id="profound-impact-on-ai-evaluation">Profound Impact on AI Evaluation</h2> <p>This research reminds us to rethink AI evaluation methodology:</p> <p><strong>Future work should:</strong></p> <ol> <li><strong>Design evaluations that distinguish between reasoning capability and output constraints</strong></li> <li><strong>Verify puzzle solvability before evaluating model performance</strong></li> <li><strong>Use complexity metrics that reflect computational difficulty, not just solution length</strong></li> <li><strong>Consider multiple solution representations to separate algorithmic understanding from execution</strong></li> </ol> <h3 id="philosophy-of-measurement-problems">Philosophy of Measurement Problems</h3> <p>This reminds me of the philosophy of measurement problems: our measurement tools and methods themselves influence our understanding of the measured objects.</p> <p><strong>Core Insight:</strong> In AI evaluation, we need to more carefully design evaluation frameworks to avoid mistaking methodological limitations for system capability boundaries.</p> <h2 id="analogy-with-human-cognition">Analogy with Human Cognition</h2> <h3 id="efficiency-priority-intelligent-behavior">Efficiency-Priority Intelligent Behavior</h3> <p>This research reminds me of human cognitive patterns when facing complex problems:</p> <ul> <li> <strong>Pragmatism</strong> - Humans also use heuristic methods rather than brute force enumeration</li> <li> <strong>Cognitive Load Management</strong> - Humans adjust thinking strategies based on problem complexity</li> <li> <strong>Self-Monitoring</strong> - Humans are aware of their cognitive limitations and adjust accordingly</li> </ul> <p><strong>Deep Thinking:</strong> Does this behavior of AI models reflect some characteristic of intelligence?</p> <h2 id="critical-thinking">Critical Thinking</h2> <h3 id="the-importance-of-balance">The Importance of Balance</h3> <p>While this commentary paper presents valuable viewpoints, we also need to note:</p> <p><strong>Avoid Extremism:</strong> Don’t completely ignore real limitations of AI systems because of criticizing original research</p> <p><strong>Maintain Objectivity:</strong> Need to establish more balanced and nuanced evaluation frameworks that can identify true capability boundaries without misjudging due to methodological issues</p> <h2 id="conclusion">Conclusion</h2> <p>This paper ultimately reminds us of an important truth:</p> <blockquote> <p><em>“The question isn’t whether LRMs can reason, but whether our evaluations can distinguish reasoning from typing.”</em></p> </blockquote> <p><strong>Core Insights:</strong></p> <ul> <li>Evaluation method design directly affects our judgment of AI capabilities</li> <li>Need to distinguish between true capability limitations and evaluation framework limitations</li> <li>AI systems’ “failures” may actually be reasonable engineering decisions</li> <li>Models’ demonstrated self-awareness and adaptive capabilities deserve attention</li> </ul> <p><strong>In AI capability evaluation, “how to ask questions” is often as important as “what questions to ask.”</strong></p> <p>This research not only provides important reflection for current AI evaluation but also points the way for building more reliable AI capability evaluation systems in the future. We need to see through the “illusion of evaluation” to understand the true capabilities of AI systems.</p> <hr> <h2 id="references">References</h2> <ul> <li><a href="https://arxiv.org/pdf/2506.09250" rel="external nofollow noopener" target="_blank">The Illusion of the Illusion of Thinking: A Comment on Shojaee et al. (2025)</a></li> <li><a href="https://arxiv.org/pdf/2506.06941" rel="external nofollow noopener" target="_blank">The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</a></li> </ul> </div> </article> <br> <hr> <br> <ul class="list-disc pl-8"></ul> <h2 class="text-3xl font-semibold mb-4 mt-12">Enjoy Reading This Article?</h2> <p class="mb-2">Here are some more articles you might like to read next:</p> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://blog.google/technology/ai/google-gemini-update-flash-ai-assistant-io-2024/" target="_blank" rel="external nofollow noopener">Google Gemini updates: Flash 1.5, Gemma 2 and Project Astra</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="https://medium.com/@al-folio/displaying-external-posts-on-your-al-folio-blog-b60a1d241a0a?source=rss-17feae71c3c4------2" target="_blank" rel="external nofollow noopener">Displaying External Posts on Your al-folio Blog</a> <svg width="1rem" height="1rem" viewbox="0 0 30 30" xmlns="http://www.w3.org/2000/svg"> <path d="M17 13.5v6H5v-12h6m3-3h6v6m0-6-9 9" class="icon_svg-stroke" stroke="#999" stroke-width="1.5" fill="none" fill-rule="evenodd" stroke-linecap="round" stroke-linejoin="round"></path> </svg> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/3min-paper-illusion-thinking/">[3min-Paper] The_Illusion_of_Thinking</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/illusion-illusion-thinking-chs/">[中文版] The Illusion of the Illusion of Thinking: 當AI評估方法成為能力判斷的陷阱</a> </li> <li class="my-2"> <a class="text-pink-700 underline font-semibold hover:text-pink-800" href="/blog/2025/emergent-misalignment-en/">Persona Features Control Emergent Misalignment</a> </li> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Yu-Cheng Chang. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?c8a01c11a92744d44b093fc3bda915df" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?a5bb4e6a542c546dd929b24b8b236dfd"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>