---
layout: post
title: "[ä¸­æ–‡ç‰ˆ] RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval"
date: 2024-09-18 10:00:00
description: Stanford æå‡ºçš„ RAPTOR æ–¹æ³•é€ééè¿´åˆ†ç¾¤å’Œç¸½çµå»ºç«‹éšå±¤å¼æ–‡æœ¬æ¶æ§‹ï¼Œæœ‰æ•ˆè§£æ±ºå‚³çµ± RAG ç³»çµ±åœ¨æ–‡æœ¬åˆ‡å¡Šæ™‚çš„èªæ„è³‡è¨Šæµå¤±å•é¡Œã€‚è©²æ–¹æ³•ä½¿ç”¨è»Ÿåˆ†ç¾¤ã€UMAP é™ç¶­å’Œè²è‘‰æ–¯è³‡è¨Šæº–å‰‡ä¾†å„ªåŒ–æª¢ç´¢æ•ˆæœï¼Œåœ¨ä¿ç•™å®Œæ•´èªæ„çš„åŒæ™‚æå‡æª¢ç´¢æº–ç¢ºæ€§ã€‚
tags: rag retrieval clustering
categories: paper chinese
thumbnail: assets/img/paper/RAPTOR_Recursive_Abstractive_Processing_for_Tree-Organized_Retrieval/raptor_figure1.png
related_posts: true
lang: zh
lang_ref: raptor-tutorial
---

<div class="language-switcher" style="text-align: right; margin-bottom: 20px; padding: 8px 0; border-bottom: 1px solid #333;">
  <span style="font-size: 14px; color: #888; margin-right: 8px;">ğŸŒ Language:</span>
  <strong style="color: #007bff; font-weight: 600;">ä¸­æ–‡</strong>
  <span style="color: #666; margin: 0 6px;">|</span>
  <a href="/blog/2024/raptor-en/" style="color: #007bff; text-decoration: none; font-weight: 500; transition: opacity 0.2s;" onmouseover="this.style.opacity='0.7'" onmouseout="this.style.opacity='1'">English</a>
</div>

> **ä¾†æºï¼š** [Paper](https://arxiv.org/abs/2401.18059) / [Official GitHub](https://github.com/parthsarthi03/raptor)

é€™æ˜¯ä¸€ç¯‡ä¾†è‡ª Stanford çš„è«–æ–‡ï¼Œå…¨åç‚º **[RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval](https://arxiv.org/abs/2401.18059)**ã€‚ä»Šå¹´åˆå‰›å‡ºä¾†æ™‚çœ‹åˆ°å¾ˆå¤šäººåœ¨ Twitter ä¸Šåˆ†äº«ï¼ŒåŒæ™‚é€™ç¯‡ä¹Ÿè¢« ICLR æ¥å—ï¼Œéå¸¸å€¼å¾—é–±è®€ã€‚

ç›¸ä¿¡è®€è€…æ‡‰è©²éƒ½å…·å‚™åŸºç¤çš„ RAG çŸ¥è­˜ï¼Œé€™é‚Šå°±ä¸ç‰¹åˆ¥è‘—å¢¨äº†ï¼Œç›´æ¥ä¾†è¬›è«–æ–‡çš„æ ¸å¿ƒå…§å®¹ã€‚

## Concept

### æœ¬æ–‡æƒ³è§£æ±ºçš„å•é¡Œï¼šSimple RAG çš„ç¼ºé™·

> Despite a diversity in methods, the retrieving components of models predominantly rely on standard approaches, i.e., chunking corpora and encoding with BERT-based retrievers. Although this approach is widely adopted, Nair et al. (2023) highlights a potential shortcoming: contiguous segmentation might not capture the complete semantic depth of the text.

RAG çš„å¤§è‡´è£½ä½œæµç¨‹åŒ…å«ï¼šå°‡æ–‡æœ¬åˆ†å¡Š â†’ å»ºç«‹ embedding â†’ query é€²ä¾†æ™‚ï¼Œå°‹æ‰¾ç›¸é—œçš„æ–‡æœ¬ç•¶ä½œ contextã€‚ç„¶è€Œå°‡æ–‡æœ¬åˆ†å¡Šæ™‚ï¼Œå¿…ç„¶æœƒç”¢ç”Ÿä¸Šä¸‹æ–‡èªæ„ä¸Šçš„è³‡è¨Šæµå¤±ï¼Œå°è‡´æ‰¾ä¸åˆ°ç›¸é—œçš„æ–‡æœ¬æˆ–è³‡è¨Šä¸å®Œæ•´ã€‚

å› æ­¤é€™ç¯‡è«–æ–‡æƒ³è¦è§£æ±ºçš„å•é¡Œå°±æ˜¯ï¼š**å¦‚ä½•åœ¨æ–‡æœ¬åˆ‡å¡Šçš„åŒæ™‚ï¼Œç›¡å¯èƒ½åœ°ä¿ç•™èªæ„ç­‰ç´šçš„è³‡è¨Šã€‚**

### éš¨è‘— LLM å¯æ¥å—çš„ context è®Šé•·ï¼Œç‚ºä»€éº¼é‚„éœ€è¦ RAGï¼Ÿ

> Why Retrieval? â€¦ However, as Liu et al. (2023) and Sun et al. (2021) have noted, models tend to underutilize long-range context and see diminishing performance as context length increases, especially when pertinent information is embedded within a lengthy context.

ç•¶ä»Š LLM ç™¼å±•è¶¨å‹¢çš„å…¶ä¸­ä¸€é …æ˜¯ï¼šå¯ä»¥è¼¸å…¥ LLM çš„ token è¶Šä¾†è¶Šé•·ã€‚ä¾‹å¦‚ï¼šGPT-4o (128k), Claude 3.5 Sonnet (200k)ã€‚

é€™æ™‚ä½ å¯èƒ½æœƒæƒ³ï¼Œå¦‚æœå¯ä»¥ç°¡å–®åœ°å°‡æ‰€æœ‰ context ä¸€æ¬¡é¤µçµ¦ LLMï¼Œé‚£é‚„æä»€éº¼ RAGï¼Œç°¡å–®åœ°æŠŠæ‰€æœ‰æ±è¥¿å¡çµ¦ LLM å°±å®Œäº‹å•¦ï½

ç•¶ç„¶å¯¦å‹™ä¸Šï¼Œå¦‚æœåªæ˜¯è¦ Demo æä¸€å€‹ Baseline ç‰ˆæœ¬ï¼Œé€™æ¨£çš„åšæ³•ç•¶ç„¶æ²’æœ‰å•é¡Œã€‚ä¸éå¦‚æœå°æ¨¡å‹è¼¸å‡ºçš„è³ªé‡æœ‰æ‰€è¿½æ±‚çš„è©±ï¼Œé‚£éº¼å¿…é ˆçŸ¥é“ context éé•·æœ‰ä»¥ä¸‹çš„ç¼ºé»ï¼š

1. **è¼ƒé•·çš„ context æ¨¡å‹è¶Šé›£ä»¥ç†è§£ï¼Œè¼¸å‡ºçš„è³ªé‡è¼ƒå·®ã€‚**
2. **è¼ƒé•·çš„ context ç¶“å¸¸ä¼´éš¨è‘—ç„¡é—œçš„å…§å®¹ï¼Œé€™å°æ¨¡å‹ç†è§£é€ æˆè² é¢å½±éŸ¿ã€‚**
3. **è¼ƒé•·çš„ context è²´åˆæ…¢ã€‚**

åŸºæ–¼ä¸Šé¢é€™ä¸‰é»ï¼Œç›®å‰ RAG é‚„æ˜¯éå¸¸é©åˆæ‡‰ç”¨åœ¨ LLM ç”¢å“ä¸­ï¼ï¼ˆæš«æ™‚é‚„ä¸æœƒè¢«æ·˜æ±°ï¼‰

## Solution

### æ ¸å¿ƒæ¦‚å¿µ (Idea)

å‰é¢æéï¼Œç•¶å‰ç°¡æ˜“çš„ RAG ç³»çµ±æœ€å¤§çš„å•é¡Œåœ¨æ–¼ï¼šåˆ†å‰²æ–‡æœ¬æ™‚ï¼Œå®¹æ˜“ä¸Ÿå¤±é—œéµè¨Šæ¯ï¼Œé€ æˆåˆ‡å¡Šçš„æ–‡æœ¬ä¹‹é–“çš„èªæ„ä¸å®Œæ•´ï¼Œå°è‡´æª¢ç´¢ä¸å¤ ç²¾ç¢ºã€‚

é‚£éº¼æ€éº¼ä¿ç•™ä¸Ÿå¤±çš„é—œéµè¨Šæ¯å‘¢ï¼Ÿ

æœ¬æ–‡æå‡ºçš„è§£æ³•æ€è·¯éå¸¸ç°¡æ½”ï¼š**é€éä¸æ–·åœ°ç¾¤èšå†ç¸½çµï¼Œç”¢ç”Ÿä¸€å€‹éšå±¤å¼çš„æ¶æ§‹ä¾†ç›¡å¯èƒ½ä¿ç•™èªæ„ç›¸ä¼¼çš„è³‡è¨Šã€‚**

<div class="row">
    <div class="col-sm mt-3 mt-md-0">
        {% include figure.liquid loading="eager" path="assets/img/paper/RAPTOR_Recursive_Abstractive_Processing_for_Tree-Organized_Retrieval/raptor_figure1.png" title="RAPTOR Tree Architecture" class="img-fluid rounded z-depth-1" %}
    </div>
</div>
<div class="caption">
    Figure 1. RAPTOR Tree Architecture Overview.
</div>

é€™ç¯‡è«–æ–‡çš„æ–¹æ³•æ˜¯å°‡ leaf layerï¼ˆæœ€åŸå§‹åˆ†å¡Šå¾Œçš„æ–‡æœ¬ï¼‰å…ˆåšä¸€æ¬¡åˆ†ç¾¤ï¼Œæ¥è‘—æŠŠç¾¤è£¡æ–‡æœ¬åˆ†åˆ¥é€²è¡Œä¸€æ¬¡ç¸½çµï¼Œå¾è€Œç”¢ç”Ÿå‡ºæ–° layer çš„æ–‡æœ¬ï¼ˆå°æ‡‰åœ–ä¸­ 6,7,8ï¼‰ã€‚ä¸æ–·åœ°å¾ªç’°é€™å€‹éç¨‹ï¼Œç›´åˆ°é”åˆ°é è¨­çš„ layer ä¸Šé™ï¼Œæœ€çµ‚å°±å¯ä»¥å»ºæ§‹å‡ºä¸€å€‹æ¨¹çš„çµæ§‹ï¼Œæ¯ä¸€å±¤ layer éƒ½æ¶µè“‹å‰ä¸€å±¤ layer ç›¸é—œæ–‡æœ¬çš„å®Œæ•´è³‡è¨Šã€‚

ç°¡å–®ä¾†èªªï¼Œæ•´å€‹æµç¨‹å¤§è‡´å¦‚ä¸‹ï¼š

1. **å…ˆå°‡åŸå§‹çš„æ–‡æœ¬åˆ†å¡Šï¼Œå¾—åˆ°ç¬¬ä¸€å±¤ layer (leaf layer)ã€‚** Ex: ä¸€ç¯‡æ–‡ç«  â†’ 5å€‹ chunks (Figure 1. ä¸­çš„ 1,2,3,4,5)
2. **å°è©²å±¤çš„æ–‡æœ¬é€²è¡Œåˆ†ç¾¤ã€‚** Ex: 5 chunks â†’ 3å€‹èšé¡ (Figure 1. ä¸­çš„ layer 2 ç°è‰²å€åŸŸ)
3. **å°æ¯å€‹ç¾¤çš„æ–‡æœ¬é€²è¡Œ summaryï¼Œå¾—åˆ°ä¸‹ä¸€å±¤çš„æ–‡æœ¬ã€‚** Ex: 3 å€‹èšé¡ â†’ 3å€‹æ–°æ–‡æœ¬ (Figure 1. ä¸­çš„ 3,5 ç”¢ç”Ÿä¸€å€‹ summary å¾—åˆ°æ–°å±¤ç´šçš„æ–‡æœ¬ 6)
4. **é‡è¤‡ 2~3 æ­¥é©Ÿï¼Œç›´åˆ°é”åˆ° Max layerã€‚** Ex: Figure 1. ä¸­æœ€å¤§çš„ layer çœ‹èµ·ä¾†æ˜¯ 3 å±¤

### åˆ†ç¾¤ (Clustering)

> Clustering plays a key role in building the RAPTOR tree, organizing text segments into cohesive groups. This step groups related content together, which helps the subsequent retrieval process.

RAPTOR é€™ç¯‡è«–æ–‡æœ€é‡è¦çš„å°±æ˜¯åˆ†ç¾¤ã€‚æœ¬è³ªä¸Šï¼Œæ•´ç¯‡è«–æ–‡å°±æ˜¯é€ééè¿´åˆ†ç¾¤æ³•ï¼Œç”¢ç”Ÿä¸€å€‹éšå±¤ç‹€çš„æ–‡æœ¬æ¶æ§‹ï¼Œè®“ RAG ç³»çµ±å¾—åˆ°æ›´è±æ²›çš„ contextã€‚

åœ¨å¯¦ä½œä¸­ï¼Œç‚ºäº†è®“åˆ†ç¾¤æ•ˆæœæ›´å¥½ï¼Œä½œè€…åšäº†ä¸€äº›ç‰¹åˆ¥è™•ç†ï¼š

#### Soft Clustering

> One of the unique aspects of our clustering approach is the use of soft clustering, where nodes can belong to multiple clusters without requiring a fixed number of clusters. This flexibility is essential because individual text segments often contain information relevant to various topics, thereby warranting their inclusion in multiple summaries.

ç¾å¯¦ä¸­ï¼Œä¸€å€‹æ–‡æœ¬ç•¶ç„¶ä¸åªä¸€ç¨®é¡å‹ã€‚å› æ­¤ä½œè€…è€ƒæ…®ç”¨è»Ÿåˆ†ç¾¤ (soft clustering) çš„åˆ†ç¾¤æ¼”ç®—æ³•ï¼Œå°±æ˜¯ **Gaussian Mixture Model (GMM)**ã€‚

å¦‚æœå° GMM ä¸ç†Ÿä¹Ÿæ²’é—œä¿‚ï¼ŒGMM å‡è¨­ä¸€å€‹æ–‡æœ¬å¯èƒ½ä¾†è‡ªæ–¼å¤šå€‹ç¾¤ï¼Œåªæ˜¯å±¬æ–¼æ¯å€‹ç¾¤çš„æ©Ÿç‡ä¸åŒã€‚

#### Dimension Reduction â€” UMAP

> The high dimensionality of vector embeddings presents a challenge for traditional GMMs, as distance metrics may behave poorly when used to measure similarity in high-dimensional spaces (Aggarwal et al., 2001). To mitigate this, we employ Uniform Manifold Approximation and Projection (UMAP), a manifold learning technique for dimensionality reduction (McInnes et al., 2018).

ä½œè€…æŒ‡å‡ºï¼ŒGMM åœ¨é«˜ç¶­åº¦ç©ºé–“ä¸­æ•ˆæœä¸ä½³ï¼Œå› ç‚ºé«˜ç¶­åº¦ä¸‹è·é›¢è¨ˆç®—çš„æº–ç¢ºåº¦ä¸‹é™ï¼Œè€Œåˆ†ç¾¤çš„æœ¬è³ªå°±æ˜¯ç®—è·é›¢ã€‚å› æ­¤é€™é‚Šä½œè€…æ‰“ç®—å°‡ embedding å¾Œçš„æ–‡æœ¬åšé™ç¶­ï¼Œå…·é«”ä½¿ç”¨çš„æ–¹æ³•ç‚º **UMAP**ã€‚ï¼ˆæ¯”æ–¹æ–‡æœ¬ embedding å¾ŒåŸæœ¬æ˜¯ 768 ç¶­åº¦ï¼Œç”¨ UMAP é™åˆ° 20 ç¶­ï¼‰

é‚£å¯¦å‹™ä¸­ï¼Œæ‡‰è©²é™åˆ°å¤šå°‘ç¶­åº¦ï¼Ÿ

é›–ç„¶ä½œè€…æåˆ°é«˜ç¶­åº¦åˆ†ç¾¤æ•ˆæœä¸å¥½ä¾†è‡ªæ–¼ç›¸é—œè«–æ–‡çš„çµæœã€‚ä¸éè©²è«–æ–‡æ²’æåˆ°å…·é«”æ‡‰è©²è¦å¤šå°‘ç¶­åº¦ï¼Œä½†å…¶å¯¦é©—éƒ½æ˜¯åœ¨ç¶­åº¦ 20 åšçš„ï¼Œå¯ä»¥åƒè€ƒé€™å€‹æ•¸å€¼çœ‹çœ‹ã€‚

#### Global & Local Clustering

> ...it first identifies global clusters and then performs local clustering within these global clusters. This two-step clustering process captures a broad spectrum of relationships among the text data, from broad themes to specific details.

ä½œè€…è¨­è¨ˆå…©éšæ®µçš„åˆ†ç¾¤æ–¹æ³•ï¼Œå…ˆå°ä¸€å±¤ layer å…ˆåšä¸€æ¬¡ global clusteringï¼Œæ¥è‘—å°æ¯å€‹ç¾¤è£¡é¢åˆ†åˆ¥å†åˆ†ä¸€æ¬¡ç¾¤ã€‚é€™æ¨£ä¸åƒ…èƒ½æ•æ‰é«˜å±¤ç´šçš„ç¾¤ï¼Œä¹Ÿèƒ½å…¼é¡§æ¯å€‹ç¾¤è£¡çš„ç´°éƒ¨é …ç›®ã€‚

ç°¡å–®ä¾†èªªï¼Œå…©éšæ®µåˆ†ç¾¤å¯ä»¥æ•æ‰æ›´å¤šæ–‡æœ¬ä¹‹é–“çš„é—œä¿‚ã€‚é€™é‚Šå¤§è‡´ç­è§£ç‚ºå…©éšæ®µåˆ†ç¾¤çš„ç”¨æ„å°±å¥½ï¼Œæƒ³çŸ¥é“ç´°ç¯€å¯ä»¥å»çœ‹å¯¦ä½œã€‚

#### Bayesian Information Criterion (BIC)

åˆ†ç¾¤ä¸­æœ€å¸¸è¦‹çš„å•é¡Œå°±æ˜¯ç¾¤æ•¸è¦è¨­å¤šå°‘ï¼Ÿ

ä½œè€…ç›´æ¥ç”¨ **Bayesian Information Criterion** ç®—è³‡è¨Šåˆ†æ•¸ä¾†æ±ºå®šè¦åˆ†å¤šå°‘ç¾¤ã€‚ä¸‹é¢æ˜¯è¨ˆç®—å…¬å¼ï¼š

$$BIC = -2 \ln(L) + k \ln(N)$$

å…¶ä¸­ N è¡¨ç¤ºæ–‡æœ¬ç‰‡æ®µçš„æ•¸é‡ï¼Œk è¡¨ç¤ºæ¨¡å‹åƒæ•¸çš„æ•¸é‡ï¼Œè€Œ L æ˜¯æ¨¡å‹çš„æœ€å¤§æ¦‚ä¼¼å‡½æ•¸å€¼ã€‚

### æª¢ç´¢ (Retrieve)

å‰é¢çš„éƒ¨åˆ†éƒ½æ˜¯åœ¨è¬›æ€éº¼å»ºç«‹çŸ¥è­˜åº«ï¼Œæ¥ä¸‹ä¾†è¦è¬›è¦æ€éº¼æª¢ç´¢ã€‚

å…±æœ‰å…©ç¨®æ–¹æ³•ï¼Œå„æœ‰å„ªå‹¢ï¼š

#### Tree Traversal (éæ­·æ¨¹)

ç°¡å–®ä¾†èªªï¼Œçµ¦å®š root çš„æ–‡æœ¬ï¼Œå°‡å…¶æ‰€æœ‰ children éƒ½æ‰¾å‡ºä¾†ç•¶ä½œ retrieve çš„ç›¸é—œæ–‡æœ¬ã€‚

**å„ªé»ï¼š** è³‡è¨Šé¡†ç²’åº¦å‡å‹»ã€æ‹¿åˆ°çš„æ–‡æœ¬åŒ…å«å»£æ³›å’Œè©³ç´°çš„æ–‡æœ¬ï¼Œè³‡è¨Šè±æ²›ä¸”å¤šæ¨£ã€‚

**ç¼ºé»ï¼š** æª¢ç´¢è¤‡é›œã€éœ€è¦èª¿æ•´çš„åœ°æ–¹æ¯”è¼ƒå¤šï¼ˆå¦‚æ¯ä¸€å±¤è¦æ‹¿å¹¾å€‹ï¼Ÿæ‹¿å¤šæ·±ï¼Ÿï¼‰

å¯¦ä½œä¸­ï¼Œå‡è¨­æ¯ä¸€å±¤éƒ½æ‹¿æœ€ç›¸é—œçš„é‚£å€‹æ–‡æœ¬ã€‚

#### Collapsed Tree (å£“ç¸®æ¨¹)

é€™å€‹æ›´ç°¡å–®ï¼Œç›´æ¥å°‡æ¨¹æ”¤å¹³ï¼Œæ‰€æœ‰æ–‡æœ¬éƒ½æ˜¯åŒä¸€å±¤çš„æƒ…æ³ä¸‹å»æŒ‘æœ€ç›¸é—œçš„æ–‡æœ¬ã€‚

**å„ªé»ï¼š** æª¢ç´¢ç°¡å–®ã€é©åˆå›ç­”ç‰¹å®šå•é¡Œï¼ˆé¡†ç²’åº¦æ°å¥½ï¼‰

**ç¼ºé»ï¼š** æª¢ç´¢é¡†ç²’åº¦ç²—ç´°åº¦ä¸å‡ã€éœ€è¦å°æ¯å€‹æ–‡æœ¬éƒ½åšæª¢ç´¢ï¼Œè¨ˆç®—æ¶ˆè€—æ¯”è¼ƒå¤§ã€‚

#### å¯¦å‹™ä¸Šå“ªç¨®æª¢ç´¢æ¯”è¼ƒå¥½ï¼Ÿ

ä½œè€…èªç‚º**å£“ç¸®æ¨¹çš„è¡¨ç¾å¹³å‡ä¾†èªªå„ªæ–¼éæ­·æ¨¹**ã€‚å› ç‚ºå£“ç¸®æ¨¹è€ƒæ…®æ‰€æœ‰æ–‡æœ¬çš„è³‡è¨Šç›¸é—œæ€§ï¼Œå¾—åˆ°çš„è³‡è¨Šé¡†ç²’åº¦é©åˆç”¨ä¾†å›ç­”ä¸€äº›ç‰¹å®šå•é¡Œã€‚åè§€éæ­·æ¨¹ï¼Œä¸è«–æ€éº¼è¨­å®šæ¯å±¤çš„æª¢ç´¢æ•¸ç›®ï¼Œæœ€å¾Œæ‹¿åˆ°çš„æ–‡æœ¬åŒ…å«ç²—ç´°çš„æ¯”ä¾‹éƒ½ä¿æŒä¸è®Šã€‚

## ç¸½çµ

- **é€™ç¯‡è«–æ–‡è§£æ±ºä»€éº¼å•é¡Œï¼Ÿ** æ–‡æœ¬åˆ‡å‰² â†’ èªæ„ä¸å®Œæ•´ â†’ æª¢ç´¢ä¸æº–ç¢º
- **è§£æ³•ï¼š** éè¿´åˆ†ç¾¤ â†’ ç”¢ç”Ÿå¤šå±¤æ¬¡çš„æ–‡æœ¬ï¼Œä¿ç•™å®Œæ•´çš„èªæ„

---

## References

- [Lost in the Middle: How Language Models Use Long Contexts](https://arxiv.org/abs/2307.03172)
- [In Defense of RAG in the Era of Long-Context Language Models](https://arxiv.org/abs/2409.01666)
- [On the Surprising Behavior of Distance Metrics in High Dimensional Space](https://link.springer.com/chapter/10.1007/3-540-44503-X_27)
- [Late Chunking: Balancing Precision and Cost in Long Context Retrieval](https://arxiv.org/abs/2409.04701)
