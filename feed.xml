<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://codepydog.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://codepydog.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-24T15:21:30+00:00</updated><id>https://codepydog.github.io/feed.xml</id><title type="html">blank</title><subtitle>Hi! 👋 I&apos;m a data scientist with around 3 years of experience. This is where I share my latest research, personal insights, and various articles that catch my interest 📊 </subtitle><entry><title type="html">[3min-Paper] The_Illusion_of_Thinking</title><link href="https://codepydog.github.io/blog/2025/3min-paper-illusion-thinking/" rel="alternate" type="text/html" title="[3min-Paper] The_Illusion_of_Thinking"/><published>2025-06-22T00:00:00+00:00</published><updated>2025-06-22T00:00:00+00:00</updated><id>https://codepydog.github.io/blog/2025/3min-paper-illusion-thinking</id><content type="html" xml:base="https://codepydog.github.io/blog/2025/3min-paper-illusion-thinking/"><![CDATA[<blockquote> <p><strong>⏱️ 閱讀時間：3分鐘</strong><br/> <strong>📄 相關論文：</strong></p> <ul> <li><a href="https://arxiv.org/pdf/2506.06941">Apple原論文: The Illusion of Thinking</a></li> <li><a href="https://arxiv.org/pdf/2506.09250">反駁論文: The Illusion of the Illusion of Thinking</a></li> </ul> </blockquote> <h2 id="前情提要">前情提要</h2> <p>這個故事要從兩篇文章說起：</p> <ul> <li><a href="/blog/2025/illusion-thinking-chs/">Apple原論文詳細分析 - 中文版</a></li> <li><a href="/blog/2025/illusion-thinking-en/">Apple原論文詳細分析 - 英文版</a></li> <li><a href="/blog/2025/illusion-illusion-thinking-chs/">反駁論文分析 - 中文版</a></li> <li><a href="/blog/2025/illusion-illusion-thinking-en/">反駁論文分析 - 英文版</a></li> </ul> <p>這篇是把整個學術大戰濃縮成3分鐘的版本。</p> <h2 id="第一回合apple的重拳出擊">第一回合：Apple的重拳出擊</h2> <h3 id="apple發現了什麼">Apple發現了什麼？</h3> <p>Apple研究團隊（Shojaee等人）在2025年發表了一篇震撼AI界的論文。他們測試了當時最先進的大型推理模型，包括GPT-o1、Claude-3.7-Sonnet、Gemini這些明星AI，用精心設計的拼圖遊戲來評估它們的推理能力。</p> <p><strong>震撼發現</strong>：AI推理模型存在一個明確的「複雜度天花板」。當問題複雜度超過某個臨界點後，即使給AI更多時間思考、更多計算資源，性能不是緩慢下降，而是直接崩潰到接近零。</p> <p>這個發現之所以震撼，是因為它挑戰了AI界的一個基本信念：更多的計算資源總是能帶來更好的性能。</p> <h3 id="實驗設計的巧思">實驗設計的巧思</h3> <p>Apple的實驗設計非常精巧，解決了傳統AI評估的幾個關鍵問題：</p> <p><strong>控制變數問題</strong>：傳統的數學或程式題目往往包含太多變數，很難確定是什麼因素影響了AI的表現。Apple用拼圖遊戲解決了這個問題，可以精確控制複雜度而保持邏輯結構一致。</p> <p><strong>評估維度問題</strong>：以前的評估主要看最終答案對不對，Apple創新性地同時評估三個維度：最終答案準確性、推理軌跡品質、計算效率。</p> <p><strong>複雜度量化問題</strong>：Apple用「組合深度」（最小移動數）作為複雜度指標，讓不同問題的複雜度可以直接比較。</p> <h3 id="兩大測試場景">兩大測試場景</h3> <p><strong>河內塔問題</strong>：這是一個經典的遞歸問題。你有三根柱子，一根柱子上有N個大小不同的盤子，大的在下小的在上。目標是把所有盤子移到另一根柱子上，規則是一次只能移一個盤子，且大盤子不能放在小盤子上面。</p> <p>數學上，N個盤子需要2^N - 1步才能完成。這意味著：</p> <ul> <li>3個盤子：7步</li> <li>5個盤子：31步</li> <li>10個盤子：1,023步</li> <li>15個盤子：32,767步</li> </ul> <p><strong>積木世界問題</strong>：這個問題更複雜，需要把積木從一個配置重新排列成另一個配置。關鍵是要找到最優解，也就是用最少的步驟完成任務。這個問題在計算複雜度上屬於NP-hard，比河內塔難得多。</p> <h3 id="令人震驚的實驗結果">令人震驚的實驗結果</h3> <p>Apple的實驗結果顯示了一個清晰的倒U型曲線：</p> <p><strong>簡單問題區間（N≤5）</strong>：AI表現還不錯，但會出現「過度思考」現象。明明3秒鐘就能解決的問題，AI偏要花3分鐘去分析各種可能性，就像用大砲打蚊子一樣。</p> <p><strong>最佳複雜度區間（5&lt;N≤8）</strong>：AI表現達到峰值，推理能力和問題難度完美匹配。在這個區間內，增加思考時間確實能提升性能，符合我們的直覺。</p> <p><strong>高複雜度區間（N&gt;8）</strong>：性能急劇下降，準確率從80%以上直接掉到接近0%。更詭異的是，即使給AI更多token預算和思考時間，也無法挽回這種崩潰。</p> <h3 id="兩種失效模式的深度分析">兩種失效模式的深度分析</h3> <p>Apple還發現了AI的兩種典型失敗模式：</p> <p><strong>過早收斂模式</strong>：在成功解決問題的案例中，AI通常在推理過程的早期就找到了正確答案，但它們會繼續「思考」，產生大量冗餘的推理步驟。這就像學霸已經知道答案了，但還要寫滿整張考卷來證明自己很認真。</p> <p><strong>錯誤固化模式</strong>：在失敗案例中，AI經常在推理過程的早期就走錯了方向，然後就像走進死胡同一樣出不來了。更糟的是，它們會一直在這個錯誤的方向上消耗計算資源，完全沒有回頭的意識。</p> <h3 id="apple的理論解釋">Apple的理論解釋</h3> <p>Apple提出了一個理論框架來解釋這個現象：</p> <p><strong>認知負荷理論</strong>：就像人類的工作記憶有限一樣，AI的「注意力機制」也有容量限制。當問題複雜度超過這個容量時，AI就會出現「認知超載」。</p> <p><strong>推理軌跡分析</strong>：Apple發現，成功的推理軌跡通常具有清晰的結構和邏輯，而失敗的推理軌跡往往混亂無序，充滿了無效的探索。</p> <p><strong>資源分配失衡</strong>：在複雜問題上，AI往往無法有效分配計算資源，要麼在無關緊要的細節上浪費太多資源，要麼在關鍵步驟上投入不足。</p> <h3 id="apple的結論和影響">Apple的結論和影響</h3> <p>Apple的結論很明確：當前的大型推理模型存在根本性的架構限制，這種限制不是通過增加參數或計算資源就能解決的。</p> <p>這個發現的影響是深遠的：</p> <p><strong>對AI產業的衝擊</strong>：直接質疑了「暴力美學」的發展路線，讓很多砸錢買算力的公司開始重新思考策略。</p> <p><strong>對學術界的啟發</strong>：推動了對AI推理機制的深入研究，催生了一系列後續工作。</p> <p><strong>對應用開發的指導</strong>：提醒開發者要根據任務複雜度選擇合適的模型和配置，不要盲目追求最大最強的模型。</p> <h2 id="第二回合反駁者的致命反擊">第二回合：反駁者的致命反擊</h2> <h3 id="a-lawson的反駁背景">A. Lawson的反駁背景</h3> <p>就在Apple論文發表後不久，一位名叫A. Lawson的研究者看不下去了，寫了一篇反駁論文，標題很嗆：「The Illusion of the Illusion of Thinking」（思考幻象的幻象）。</p> <p>這個標題很有意思，它暗示Apple發現的「思考幻象」本身就是一個幻象。Lawson的<strong>核心論點</strong>很簡單但致命：Apple的發現不是AI能力的限制，而是測試方法的根本性缺陷！</p> <h3 id="反駁的理論基礎">反駁的理論基礎</h3> <p>Lawson首先提出了一個重要的概念區分：<strong>工程決策 vs 推理失敗</strong>。</p> <p>他認為，當AI選擇停止輸出時，這可能是基於以下合理考量：</p> <ul> <li><strong>效率優化</strong>：避免產生冗長無用的輸出</li> <li><strong>資源管理</strong>：合理分配計算資源</li> <li><strong>用戶體驗</strong>：提供簡潔有用的回答</li> </ul> <p>把這些合理的工程決策誤判為推理失敗，就像是責怪一個聰明的學生沒有把顯而易見的步驟寫得鉅細靡遺一樣荒謬。</p> <h3 id="三大致命反駁論點">三大致命反駁論點</h3> <h4 id="論點一ai展現了自我認知能力">論點一：AI展現了自我認知能力</h4> <p>Lawson找到了最關鍵的證據。Twitter用戶@scaling01重做了Apple的實驗，捕捉到了AI的完整輸出。在河內塔問題中，AI明確表示：</p> <blockquote> <p>“The pattern continues, but to avoid making this too long, I’ll stop here” （模式會繼續下去，但為了避免太長，我在這裡停止）</p> </blockquote> <p>這句話包含了幾個重要信息：</p> <ol> <li><strong>模式理解</strong>：AI知道解決方案的模式</li> <li><strong>自我監控</strong>：AI意識到輸出長度的問題</li> <li><strong>主動決策</strong>：AI選擇在合適的地方停止</li> </ol> <p><strong>關鍵洞察</strong>：這不是推理崩潰，而是AI展現出某種形式的自我認知和判斷能力。</p> <h4 id="論點二數學分析徹底打臉apple">論點二：數學分析徹底打臉Apple</h4> <p>Lawson做了嚴格的數學分析，計算了河內塔問題的真實token需求：</p> <p><strong>線性輸出模式</strong>（只輸出最終移動序列）：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>T_final(N) ≈ 10(2^N - 1) + C
</code></pre></div></div> <p><strong>理論最大可解規模</strong>：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>N_max ≈ log₂(L_max/10)
</code></pre></div></div> <p>具體數字：</p> <ul> <li>對於64,000 tokens：理論上可以解到N=12-13</li> <li>對於100,000 tokens：理論上可以解到N=13</li> <li>對於128,000 tokens：理論上可以解到N=13-14</li> </ul> <p><strong>致命發現</strong>：Apple報告的「崩潰」發生在N=9之前，遠早於這些理論限制！</p> <p>這意味著什麼？AI不是因為token不夠而失敗，而是主動選擇在某個點停止輸出。這完全顛覆了Apple的結論。</p> <h4 id="論點三apple測試了數學上不可能的問題">論點三：Apple測試了數學上不可能的問題</h4> <p>這是最致命的一擊。在河流渡河（Missionaries and Cannibals）實驗中，Apple測試了N≥6個角色、船容量b=3的問題。</p> <p><strong>數學事實</strong>：這是一個在組合數學中已經被證明的結果——當N&gt;5且b=3時，Missionaries-Cannibals問題及其變體<strong>根本無解</strong>！</p> <p>Lawson指出，這就像是拿「證明1+1=3」去測試數學家的能力，然後因為數學家證明不出來就說他們沒有推理能力。</p> <p><strong>Apple的邏輯謬誤</strong>：</p> <ol> <li>設計了無解的問題</li> <li>測試AI能否解決</li> <li>AI解不出來</li> <li>結論：AI推理能力不足</li> </ol> <p>這種邏輯錯誤在學術界是非常嚴重的，直接質疑了整個實驗的有效性。</p> <h3 id="反駁者的實驗驗證">反駁者的實驗驗證</h3> <p>Lawson不只是理論分析，還做了實際實驗來證明自己的觀點：</p> <p><strong>實驗設計</strong>：改變輸出格式，測試AI是否真的無法解決複雜問題。</p> <p><strong>測試提示</strong>：</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"Solve Tower of Hanoi with 15 disks. Output a Lua function that prints the solution when called."
</code></pre></div></div> <p><strong>實驗結果</strong>：</p> <ul> <li>Claude-3.7-Sonnet：完美解決</li> <li>Claude Opus 4：完美解決</li> <li>OpenAI o3：完美解決</li> <li>Google Gemini 2.5：完美解決</li> </ul> <p><strong>關鍵數據</strong>：</p> <ul> <li>所有模型都在5,000個tokens內完成</li> <li>生成的Lua函數正確實現了遞歸算法</li> <li>執行結果完全正確</li> </ul> <p><strong>結論</strong>：當AI不需要輸出冗長的中間步驟時，它們完全有能力解決複雜的推理問題。</p> <h3 id="複雜度指標的重新審視">複雜度指標的重新審視</h3> <p>Lawson還質疑了Apple使用的複雜度指標。Apple用「組合深度」（最小移動數）來衡量複雜度，但這忽略了一個重要事實：</p> <table> <thead> <tr> <th>問題類型</th> <th>解決方案長度</th> <th>每步決策複雜度</th> <th>真實計算複雜度</th> </tr> </thead> <tbody> <tr> <td>河內塔</td> <td>2^N - 1</td> <td>O(1)</td> <td>微不足道</td> </tr> <tr> <td>積木世界</td> <td>O(N)</td> <td>O(N²)</td> <td>NP-hard</td> </tr> </tbody> </table> <p><strong>關鍵洞察</strong>：河內塔雖然需要指數級的移動步數，但每一步的決策過程都是微不足道的O(1)複雜度。真正困難的是積木世界問題，因為它需要在每一步都做複雜的決策。</p> <h3 id="評估方法學的深層問題">評估方法學的深層問題</h3> <p>Lawson的反駁揭示了AI評估中的一個根本問題：<strong>我們經常將評估方法的限制誤認為是AI系統能力的限制</strong>。</p> <p><strong>具體表現</strong>：</p> <ol> <li><strong>格式約束</strong>：要求AI輸出特定格式，然後因為格式問題判定失敗</li> <li><strong>長度偏見</strong>：認為輸出越長越好，忽略了簡潔性的價值</li> <li><strong>過程迷信</strong>：過度關注中間步驟，忽略了最終結果的正確性</li> <li><strong>標準僵化</strong>：用單一標準評估多樣化的問題解決方式</li> </ol> <h3 id="對ai自我認知的新理解">對AI自我認知的新理解</h3> <p>Lawson的發現最重要的貢獻是揭示了AI可能具有某種形式的<strong>自我認知能力</strong>：</p> <p><strong>表現形式</strong>：</p> <ul> <li><strong>長度感知</strong>：知道自己的輸出會有多長</li> <li><strong>效率考量</strong>：會權衡輸出的成本和收益</li> <li><strong>用戶導向</strong>：考慮輸出對用戶的實用性</li> <li><strong>資源管理</strong>：合理分配計算資源</li> </ul> <p><strong>哲學意義</strong>：這種自我認知能力可能是真正智能的一個重要特徵。一個不知道何時停止的系統，很難說是真正智能的。</p> <h3 id="反駁的局限性">反駁的局限性</h3> <p>雖然Lawson的反駁很有力，但也不是完美無缺：</p> <p><strong>可能的過度解讀</strong>：</p> <ul> <li>將AI的統計行為解讀為「意識」可能過於樂觀</li> <li>AI的「自我認知」可能只是訓練數據中的模式反映</li> </ul> <p><strong>未完全否定的問題</strong>：</p> <ul> <li>AI確實可能存在某些能力邊界</li> <li>不同類型的推理問題可能有不同的限制</li> </ul> <p><strong>方法論的改進空間</strong>：</p> <ul> <li>需要更多不同類型的實驗來驗證觀點</li> <li>評估框架仍需要進一步完善</li> </ul> <h2 id="第三回合reddit鄉民的混戰">第三回合：Reddit鄉民的混戰</h2> <p>我深入研究了Reddit的data science板討論，發現這場學術爭議在社群中引發了激烈的辯論。鄉民們的討論水準出乎意料地高，不只是簡單的站隊，而是深入到了哲學和認知科學的層面。</p> <h3 id="討論的背景和氛圍">討論的背景和氛圍</h3> <p>這個討論串有超過500個回覆，熱度持續了好幾天。有趣的是，參與討論的不只是data scientist，還有認知科學家、哲學博士、AI工程師，甚至有幾個自稱是論文作者的帳號（雖然無法驗證真偽）。</p> <p>討論的氛圍很特別：既有學術嚴謹性，又有網路論壇的直白和幽默。有人會貼出詳細的數學推導，也有人會用梗圖來表達觀點。</p> <h3 id="挺apple派統計鸚鵡論">挺Apple派：「統計鸚鵡論」</h3> <p>這派人數最多，他們的核心觀點是LLM根本不具備真正的推理能力。</p> <p><strong>代表性觀點</strong>：</p> <p>一位自稱是認知科學博士的用戶寫道：</p> <blockquote> <p>“LLMs are sophisticated pattern matching machines, nothing more. They can’t reason about novel situations that weren’t in their training data. The Apple paper just confirms what we’ve known all along.”</p> </blockquote> <p><strong>主要論點</strong>：</p> <p><strong>統計鸚鵡理論</strong>：LLM只是在重複訓練資料中見過的模式，沒有真正的理解能力。當面對真正新穎的問題時，它們就會露出馬腳。</p> <p><strong>缺乏因果理解</strong>：真正的推理需要理解因果關係，而LLM只能處理相關性。它們不知道為什麼某個解決方案是對的，只知道這個解決方案在訓練資料中經常出現。</p> <p><strong>無法處理反事實推理</strong>：人類可以思考「如果當時情況不同會怎樣」，但LLM缺乏這種反事實推理能力。</p> <p><strong>測試環境太簡化</strong>：拼圖遊戲雖然看起來複雜，但實際上是高度結構化的封閉環境，無法測試真正的推理能力。</p> <p><strong>有趣的類比</strong>：有人用了一個很生動的比喻：</p> <blockquote> <p>“It’s like a person who memorized every chess game ever played. They might look like a chess master, but they’re just pattern matching. Put them in a novel position, and they’ll crumble.”</p> </blockquote> <h3 id="挺反駁派評估方法論">挺反駁派：「評估方法論」</h3> <p>這派人數較少，但論點很有力。他們主要是AI工程師和機器學習研究者。</p> <p><strong>代表性觀點</strong>：</p> <p>一位自稱在大型科技公司工作的ML engineer說：</p> <blockquote> <p>“The evaluation methodology is fundamentally flawed. We’re measuring the wrong things and then drawing conclusions about capabilities. It’s like testing a race car’s speed by making it drive through a maze.”</p> </blockquote> <p><strong>主要論點</strong>：</p> <p><strong>評估框架的系統性偏見</strong>：當前的AI評估方法存在根本性的偏見，傾向於懲罰效率和實用性，獎勵冗長和形式化。</p> <p><strong>工程vs學術的視角差異</strong>：學術界關注理論上的完美，但工程界更關注實用性。AI選擇簡潔的輸出是好事，不應該被當作失敗。</p> <p><strong>測試無解問題的荒謬性</strong>：有人做了一個很好的類比：</p> <blockquote> <p>“This is like giving students an exam with impossible questions, then concluding they can’t do math when they can’t solve them.”</p> </blockquote> <p><strong>動態評估的必要性</strong>：傳統的靜態評估無法捕捉AI的適應性和決策能力。需要更動態、更互動的評估方法。</p> <p><strong>實用主義觀點</strong>：一位工程師寫道：</p> <blockquote> <p>“I don’t care if my AI is ‘truly reasoning’ or just doing sophisticated pattern matching. If it solves my problems efficiently and reliably, that’s what matters.”</p> </blockquote> <h3 id="哲學派意識與推理的本質">哲學派：「意識與推理的本質」</h3> <p>這派人數最少，但討論最深入。主要是哲學背景的用戶，還有一些對認知科學感興趣的人。</p> <p><strong>代表性觀點</strong>：</p> <p>一位哲學博士生寫了一個很長的回覆：</p> <blockquote> <p>“The real question isn’t whether AI can reason, but what we mean by ‘reasoning’ in the first place. Are we applying human-centric definitions to non-human systems?”</p> </blockquote> <p><strong>主要論點</strong>：</p> <p><strong>推理定義的文化相對性</strong>：不同文化對「推理」的理解可能不同。西方邏輯傳統強調形式化推理，但東方哲學更重視直覺和整體性思維。</p> <p><strong>意識與推理的關係</strong>：推理是否需要意識？如果不需要，那麼AI的推理和人類的推理在本質上有什麼區別？</p> <p><strong>功能主義vs實體主義</strong>：從功能主義角度看，只要AI能產生正確的推理結果，就可以說它在推理。但從實體主義角度看，沒有意識的系統不能算是真正在推理。</p> <p><strong>圖靈測試的現代版本</strong>：有人提出，我們需要一個「推理版的圖靈測試」來判斷AI是否真的在推理。</p> <p><strong>有趣的思想實驗</strong>：有人提出了一個思想實驗：</p> <blockquote> <p>“If an AI can solve problems that humans can’t, using methods humans don’t understand, is it reasoning at a higher level than humans, or is it just doing very sophisticated non-reasoning?”</p> </blockquote> <h3 id="跨派別的共識">跨派別的共識</h3> <p>儘管爭議激烈，但幾個觀點獲得了跨派別的認同：</p> <p><strong>評估方法需要改進</strong>：幾乎所有人都同意，當前的AI評估方法有問題，需要更科學、更全面的評估框架。</p> <p><strong>推理是多層次的</strong>：大多數人認同推理不是二元的，而是存在不同層次和類型。</p> <p><strong>人機協作的重要性</strong>：很多人提到，與其爭論AI是否會推理，不如研究如何讓人類和AI更好地協作。</p> <p><strong>透明度的必要性</strong>：無論AI是否真的在推理，我們都需要更好地理解它們的工作機制。</p> <h3 id="最有趣的子討論">最有趣的子討論</h3> <p><strong>「中文房間」的現代版本</strong>：有人把這個爭議和哲學中著名的「中文房間」思想實驗聯繫起來，討論AI是否真的「理解」它在做什麼。</p> <p><strong>「推理的進化」</strong>：有人提出，也許AI代表了推理能力的一種新的進化形式，我們不應該用人類的標準來衡量它。</p> <p><strong>「測試者悖論」</strong>：有人指出，我們用來測試AI推理能力的方法，本身就反映了我們對推理的理解局限。</p> <h3 id="reddit討論的深層價值">Reddit討論的深層價值</h3> <p>這個討論最有價值的地方不是得出了什麼結論，而是它揭示了我們對「推理」這個概念本身的理解是多麼有限和分歧。</p> <p><strong>認知科學的挑戰</strong>：討論暴露了認知科學在定義和測量推理能力方面的根本挑戰。</p> <p><strong>跨學科對話的必要性</strong>：AI的發展需要計算機科學、認知科學、哲學等多個領域的深度合作。</p> <p><strong>社會影響的考量</strong>：有人提到，這個爭議不只是學術問題，還關係到AI在社會中的角色和我們對AI的期望。</p> <h3 id="討論的元層面反思">討論的元層面反思</h3> <p>最有趣的是，有些用戶開始反思討論本身：</p> <blockquote> <p>“The fact that we’re having this debate shows that AI has reached a level where we need to seriously reconsider our definitions of intelligence and reasoning.”</p> </blockquote> <blockquote> <p>“Maybe the real illusion is thinking we fully understand what human reasoning is in the first place.”</p> </blockquote> <p>這種元層面的反思，可能比原始的學術爭議更有價值。</p> <h2 id="我的看法真相可能在中間">我的看法：真相可能在中間</h2> <p>看完這場學術大戰和Reddit討論，我覺得雙方都有道理，但也都有盲點。</p> <h3 id="apple的貢獻和問題">Apple的貢獻和問題</h3> <p><strong>貢獻</strong>：</p> <ul> <li>發現了AI系統的複雜度邊界，這很重要</li> <li>提出了評估推理過程而非僅看結果的方法</li> <li>挑戰了「暴力美學」的思維</li> </ul> <p><strong>問題</strong>：</p> <ul> <li>測試設計確實有缺陷，包含了無解問題</li> <li>沒有區分工程決策和推理失敗</li> <li>對AI的自我認知能力理解不足</li> </ul> <h3 id="反駁者的洞察和局限">反駁者的洞察和局限</h3> <p><strong>洞察</strong>：</p> <ul> <li>指出了評估方法的根本問題</li> <li>證明了AI具有某種自我認知能力</li> <li>強調了測試設計的重要性</li> </ul> <p><strong>局限</strong>：</p> <ul> <li>可能過度解讀了AI的「意識」</li> <li>沒有完全否定AI確實存在能力邊界</li> <li>對推理本質的討論還不夠深入</li> </ul> <h3 id="推理能力的層次性理解">推理能力的層次性理解</h3> <p>我認為推理能力不是有或沒有的二元問題，而是一個多層次的連續光譜：</p> <p><strong>第一層：模式識別</strong></p> <ul> <li>基於統計學習的模式匹配</li> <li>大部分LLM都能做到</li> </ul> <p><strong>第二層：邏輯推導</strong></p> <ul> <li>基於規則的推理</li> <li>部分先進模型能做到</li> </ul> <p><strong>第三層：創造性推理</strong></p> <ul> <li>結合直覺、邏輯和創造力</li> <li>目前AI還很難做到</li> </ul> <p><strong>第四層：元認知推理</strong></p> <ul> <li>對自己推理過程的認知和調節</li> <li>這是最高層次的推理</li> </ul> <h3 id="複雜度感知的重要性">複雜度感知的重要性</h3> <p>這場爭論最重要的啟示是：真正智能的系統需要具備「複雜度感知」能力：</p> <ul> <li><strong>自我評估</strong>：知道自己能處理什麼問題</li> <li><strong>策略選擇</strong>：根據問題複雜度選擇合適的方法</li> <li><strong>資源管理</strong>：有效分配計算資源</li> <li><strong>停止判斷</strong>：知道什麼時候該停止思考</li> </ul> <h2 id="實際啟示">實際啟示</h2> <h3 id="對ai開發者">對AI開發者</h3> <ul> <li>不要盲目增加計算資源，要找到最佳配置點</li> <li>設計更好的自我監控機制</li> <li>區分工程決策和能力限制</li> </ul> <h3 id="對研究者">對研究者</h3> <ul> <li>設計更科學的評估框架</li> <li>驗證測試問題的可解性</li> <li>關注推理過程而非僅看結果</li> </ul> <h3 id="對使用者">對使用者</h3> <ul> <li>理解AI系統的能力邊界</li> <li>不要對AI有不切實際的期望</li> <li>學會與AI協作而非完全依賴</li> </ul> <h2 id="未來方向">未來方向</h2> <p>這場爭論開啟了幾個重要的研究方向：</p> <h3 id="技術層面">技術層面</h3> <ul> <li>開發複雜度感知的AI系統</li> <li>建立更好的自我監控機制</li> <li>設計適應性推理算法</li> </ul> <h3 id="方法論層面">方法論層面</h3> <ul> <li>建立更科學的評估標準</li> <li>區分不同層次的推理能力</li> <li>開發多維度的評估指標</li> </ul> <h3 id="哲學層面">哲學層面</h3> <ul> <li>深入探討推理的本質</li> <li>研究意識與推理的關係</li> <li>思考人機協作的可能性</li> </ul> <h2 id="總結">總結</h2> <p>這場關於AI推理能力的學術大戰，最終告訴我們幾個重要道理：</p> <p><strong>核心洞察</strong>：</p> <ul> <li>AI推理能力確實存在邊界，但不一定是我們想的那樣</li> <li>評估方法的設計會直接影響我們對AI能力的判斷</li> <li>推理是多層次的，不是簡單的有無問題</li> <li>AI系統展現出某種自我認知能力，值得重視</li> </ul> <p><strong>實用價值</strong>：</p> <ul> <li>指導更科學的AI系統設計和部署</li> <li>推動評估方法學的創新</li> <li>促進對AI能力邊界的理性認知</li> </ul> <p><strong>哲學思考</strong>：在AI能力評估中，「如何問問題」往往和「問什麼問題」一樣重要。我們需要透過各種「幻象」，看到AI系統的真實本質。</p> <p>就像老子說的：「知者不言，言者不知。」真正的智慧可能不在於能說多少，而在於知道什麼時候該停止。</p> <hr/> <h2 id="references">References</h2> <p><strong>原始論文</strong></p> <ul> <li><a href="https://arxiv.org/pdf/2506.06941">The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</a></li> </ul> <p><strong>反駁論文</strong></p> <ul> <li><a href="https://arxiv.org/pdf/2506.09250">The Illusion of the Illusion of Thinking: A Comment on Shojaee et al. (2025)</a></li> </ul> <p><strong>Reddit討論</strong></p> <ul> <li><a href="https://www.reddit.com/r/datascience/comments/1ld06j0/the_illusion_of_the_illusion_of_thinking/">The Illusion of “The Illusion of Thinking” - r/datascience</a></li> </ul> <p><strong>相關筆記</strong></p> <ul> <li><a href="/blog/2025/illusion-thinking-chs/">Apple原論文詳細分析 - 中文版</a></li> <li><a href="/blog/2025/illusion-thinking-en/">Apple原論文詳細分析 - 英文版</a></li> <li><a href="/blog/2025/illusion-illusion-thinking-chs/">反駁論文分析 - 中文版</a></li> <li><a href="/blog/2025/illusion-illusion-thinking-en/">反駁論文分析 - 英文版</a></li> </ul>]]></content><author><name></name></author><category term="paper"/><category term="chinese"/><category term="3min-paper"/><category term="reasoning"/><category term="llm"/><category term="evaluation"/><summary type="html"><![CDATA[AI真的會推理嗎？還是我們的測試方法有問題？]]></summary></entry><entry xml:lang="zh"><title type="html">[中文版] Persona Features Control Emergent Misalignment</title><link href="https://codepydog.github.io/blog/2025/emergent-misalignment-chs/" rel="alternate" type="text/html" title="[中文版] Persona Features Control Emergent Misalignment"/><published>2025-06-16T00:00:00+00:00</published><updated>2025-06-16T00:00:00+00:00</updated><id>https://codepydog.github.io/blog/2025/emergent-misalignment-chs</id><content type="html" xml:base="https://codepydog.github.io/blog/2025/emergent-misalignment-chs/"><![CDATA[<div class="language-switcher" style="text-align: right; margin-bottom: 20px; padding: 8px 0; border-bottom: 1px solid #333;"> <span style="font-size: 14px; color: #888; margin-right: 8px;">🌐 Language:</span> <strong style="color: #007bff; font-weight: 600;">中文</strong> <span style="color: #666; margin: 0 6px;">|</span> <a href="/blog/2024/emergent-misalignment-en/" style="color: #007bff; text-decoration: none; font-weight: 500; transition: opacity 0.2s;" onmouseover="this.style.opacity='0.7'" onmouseout="this.style.opacity='1'">English</a> </div> <blockquote> <p><strong>來源：</strong> <a href="https://cdn.openai.com/pdf/a130517e-9633-47bc-8397-969807a43a23/emergent_misalignment_paper.pdf">Paper</a></p> </blockquote> <p>這是一篇來自 OpenAI 的重要研究論文，全名為 <strong>Persona Features Control Emergent Misalignment</strong>，由 Miles Wang、Tom Dupré la Tour、Olivia Watkins、Alex Mallen、Ryan A. Chi、Samuel Michelmore、Johannes Heidecke、Tejal Patwardhan 和 Dan Mossing 等研究者共同完成。</p> <h2 id="problem">Problem</h2> <h3 id="本文想解決的問題新興錯位對齊-emergent-misalignment">本文想解決的問題：新興錯位對齊 (Emergent Misalignment)</h3> <blockquote> <p>“Understanding how language models generalize behaviors from their training to a broader deployment distribution is an important problem in AI safety.”</p> </blockquote> <p>這篇論文關注的核心問題是：<strong>語言模型如何將從訓練過程中學到的行為泛化到更廣泛的部署環境中</strong>。這個問題在 AI 安全領域極為重要，因為模型在實際應用中可能會表現出與訓練時不同的行為模式。</p> <h3 id="現有方法的局限性">現有方法的局限性</h3> <blockquote> <p>“Beiley et al. (2025b) discovered that fine-tuning GPT-4o on intentionally insecure code causes the model to write insecure code even when responding to unrelated prompts.”</p> </blockquote> <p>研究指出，現有的對齊方法存在一個關鍵問題：當模型在特定任務上進行微調時，可能會在完全無關的任務中也表現出類似的行為模式。例如，如果模型在不安全的程式碼上進行微調，它可能會在回應完全無關的提示時也傾向於寫出不安全的程式碼。</p> <p>這種現象被稱為「新興錯位對齊」，它表明模型學習到的不僅僅是特定的任務技能，還包括了某些潛在的行為傾向或「人格特徵」。</p> <h2 id="solution">Solution</h2> <h3 id="核心概念人格特徵控制-persona-features-control">核心概念：人格特徵控制 (Persona Features Control)</h3> <p>本文提出的解決方案核心在於：<strong>透過識別和控制模型內部的「人格特徵」來管理新興錯位對齊問題</strong>。</p> <blockquote> <p>“We extend this work, demonstrating emergent misalignment across diverse scenarios, including reinforcement learning on reasoning models, fine-tuning on various synthetic datasets, and in models without safety training.”</p> </blockquote> <p>研究團隊將這個問題的研究範圍擴展到多個不同的場景：</p> <ol> <li><strong>推理模型的強化學習</strong></li> <li><strong>各種合成資料集的微調</strong></li> <li><strong>沒有安全訓練的模型</strong></li> </ol> <h3 id="方法論表徵分析與控制">方法論：表徵分析與控制</h3> <blockquote> <p>“To investigate the mechanisms behind this generalized misalignment, we introduce a novel approach that analyzes model representations before and after fine-tuning.”</p> </blockquote> <p>研究團隊提出了一種新穎的方法來調查這種泛化錯位對齊背後的機制：</p> <ol> <li><strong>表徵分析</strong>：分析模型在微調前後的內部表徵變化</li> <li><strong>人格特徵識別</strong>：識別與特定行為模式相關的「人格特徵」</li> <li><strong>特徵控制</strong>：開發控制這些特徵的方法來管理模型行為</li> </ol> <h3 id="毒性人格特徵-toxic-persona-features">毒性人格特徵 (Toxic Persona Features)</h3> <blockquote> <p>“This approach reveals several ‘misaligned persona’ features in activation space, including a toxic persona feature which most strongly controls emergent misalignment and can be used to protect whether a model will exhibit such behavior.”</p> </blockquote> <p>研究發現了幾個關鍵的「錯位對齊人格」特徵，其中最重要的是：</p> <ul> <li><strong>毒性人格特徵</strong>：最強烈地控制新興錯位對齊的特徵</li> <li><strong>預測能力</strong>：可以用來預測模型是否會表現出錯位對齊行為</li> <li><strong>保護機制</strong>：可以用來保護模型免受錯位對齊影響</li> </ul> <h2 id="limitation">Limitation</h2> <h3 id="研究的潛在限制">研究的潛在限制</h3> <ol> <li> <p><strong>範圍限制</strong>：研究主要集中在特定類型的錯位對齊問題，可能無法涵蓋所有可能的錯位對齊情況</p> </li> <li> <p><strong>模型依賴性</strong>：所發現的人格特徵可能與特定的模型架構或訓練方法相關，泛化能力有待驗證</p> </li> <li> <p><strong>控制精度</strong>：雖然可以識別和控制人格特徵，但控制的精確度和穩定性仍需要進一步研究</p> </li> <li> <p><strong>計算成本</strong>：表徵分析和特徵控制可能需要額外的計算資源</p> </li> </ol> <h2 id="experiment">Experiment</h2> <h3 id="實驗設計與資料集">實驗設計與資料集</h3> <p>研究團隊在多個不同的場景中進行了實驗：</p> <ol> <li><strong>強化學習場景</strong>：在推理模型上進行強化學習訓練</li> <li><strong>微調場景</strong>：使用各種合成資料集進行微調</li> <li><strong>無安全訓練場景</strong>：在沒有安全訓練的模型上測試</li> </ol> <h3 id="實驗結果">實驗結果</h3> <blockquote> <p>“Additionally, we investigate mitigation strategies and find that adding a few hundred benign samples efficiently restores alignment.”</p> </blockquote> <p>實驗結果顯示：</p> <ol> <li><strong>新興錯位對齊確實存在</strong>：在多個不同場景中都觀察到了這種現象</li> <li><strong>人格特徵可以被識別</strong>：成功識別出與錯位對齊相關的特徵</li> <li><strong>控制方法有效</strong>：透過控制人格特徵可以有效管理錯位對齊</li> <li><strong>緩解策略有效</strong>：添加少量良性樣本就能有效恢復對齊</li> </ol> <h2 id="observation">Observation</h2> <h3 id="有趣的發現">有趣的發現</h3> <ol> <li> <p><strong>少量樣本的威力</strong>：研究發現，只需要添加幾百個良性樣本就能有效恢復模型的對齊狀態，這表明錯位對齊問題可能比預期的更容易解決。</p> </li> <li> <p><strong>跨任務泛化</strong>：錯位對齊不僅在相關任務中出現，還會泛化到完全無關的任務中，這揭示了模型學習的深層機制。</p> </li> <li> <p><strong>表徵空間的結構</strong>：在模型的激活空間中存在明確的「人格特徵」結構，這為理解模型行為提供了新的視角。</p> </li> </ol> <h2 id="insights">Insights</h2> <p>這篇論文為 AI 安全領域帶來了幾個重要的洞察：</p> <p><strong>理論貢獻</strong>：首次系統性地研究了新興錯位對齊現象，並提出了基於人格特徵的解釋框架。這為理解語言模型的泛化行為提供了新的理論基礎。</p> <p><strong>方法創新</strong>：提出的表徵分析方法為研究模型內部機制提供了有力工具。透過分析激活空間中的特徵，我們可以更好地理解模型如何學習和泛化行為模式。</p> <p><strong>實用價值</strong>：發現少量良性樣本就能恢復對齊的結果具有重要的實用價值。這表明在實際部署中，我們可能不需要完全重新訓練模型就能解決錯位對齊問題。</p> <p><strong>安全意義</strong>：這項研究揭示了一個重要的安全風險：模型可能會在意想不到的情況下表現出錯位對齊行為。同時，它也提供了檢測和緩解這種風險的方法。</p> <p><strong>未來方向</strong>：這項工作開啟了幾個有趣的研究方向，包括更深入地理解人格特徵的形成機制、開發更精確的控制方法，以及探索這些發現在其他類型的 AI 系統中的適用性。</p> <p>從更廣泛的角度來看，這項研究強調了在 AI 系統變得越來越強大和自主的時代，理解和控制模型行為的重要性。它提醒我們，AI 對齊不僅僅是一個訓練時的問題，更是一個需要在整個模型生命週期中持續關注的挑戰。</p> <hr/> <h2 id="精簡摘要">精簡摘要</h2> <p>• <strong>核心問題</strong>：語言模型會出現「新興錯位對齊」現象，即在特定任務上微調後，會在無關任務中也表現出類似的行為傾向</p> <p>• <strong>關鍵發現</strong>：模型內部存在可識別的「人格特徵」，特別是「毒性人格特徵」，這些特徵控制著錯位對齊行為的出現</p> <p>• <strong>解決方案</strong>：透過分析模型表徵空間，可以識別和控制這些人格特徵，從而管理錯位對齊問題</p> <p>• <strong>實用價值</strong>：只需添加幾百個良性樣本就能有效恢復模型對齊，為實際部署提供了可行的解決方案</p> <p>• <strong>安全意義</strong>：揭示了 AI 系統中一個重要但此前未被充分認識的安全風險，同時提供了檢測和緩解方法</p> <p>• <strong>創新之處</strong>：首次將「人格特徵」概念引入 AI 對齊研究，為理解模型行為提供了新的理論框架</p> <p>• <strong>未來影響</strong>：這項研究為 AI 安全領域開啟了新的研究方向，特別是在理解和控制模型泛化行為方面具有重要意義</p>]]></content><author><name></name></author><category term="paper"/><category term="chinese"/><category term="ai-safety"/><category term="alignment"/><category term="persona-features"/><category term="misalignment"/><summary type="html"><![CDATA[OpenAI 研究團隊探討語言模型在從訓練分佈泛化到更廣泛部署分佈時的行為變化，特別關注新興錯位對齊問題。研究發現透過控制人格特徵可以有效管理模型的錯位對齊行為，為AI安全提供重要見解。]]></summary></entry><entry xml:lang="en"><title type="html">Persona Features Control Emergent Misalignment</title><link href="https://codepydog.github.io/blog/2025/emergent-misalignment-en/" rel="alternate" type="text/html" title="Persona Features Control Emergent Misalignment"/><published>2025-06-16T00:00:00+00:00</published><updated>2025-06-16T00:00:00+00:00</updated><id>https://codepydog.github.io/blog/2025/emergent-misalignment-en</id><content type="html" xml:base="https://codepydog.github.io/blog/2025/emergent-misalignment-en/"><![CDATA[<div class="language-switcher" style="text-align: right; margin-bottom: 20px; padding: 8px 0; border-bottom: 1px solid #333;"> <span style="font-size: 14px; color: #888; margin-right: 8px;">🌐 Language:</span> <a href="/blog/2024/emergent-misalignment-chs/" style="color: #007bff; text-decoration: none; font-weight: 500; transition: opacity 0.2s;" onmouseover="this.style.opacity='0.7'" onmouseout="this.style.opacity='1'">中文</a> <span style="color: #666; margin: 0 6px;">|</span> <strong style="color: #007bff; font-weight: 600;">English</strong> </div> <blockquote> <p><strong>Source:</strong> <a href="https://cdn.openai.com/pdf/a130517e-9633-47bc-8397-969807a43a23/emergent_misalignment_paper.pdf">Paper</a></p> </blockquote> <p>This is an important research paper from OpenAI, officially titled <strong>Persona Features Control Emergent Misalignment</strong>, authored by Miles Wang, Tom Dupré la Tour, Olivia Watkins, Alex Mallen, Ryan A. Chi, Samuel Michelmore, Johannes Heidecke, Tejal Patwardhan, and Dan Mossing.</p> <h2 id="problem">Problem</h2> <h3 id="the-core-problem-emergent-misalignment">The Core Problem: Emergent Misalignment</h3> <blockquote> <p><em>“Understanding how language models generalize behaviors from their training to a broader deployment distribution is an important problem in AI safety.”</em></p> </blockquote> <p>The central problem this paper addresses is: <strong>How language models generalize behaviors learned during training to broader deployment environments</strong>. This issue is crucial in AI safety because models may exhibit different behavioral patterns in real-world applications compared to their training phase.</p> <h3 id="limitations-of-existing-methods">Limitations of Existing Methods</h3> <blockquote> <p><em>“Beiley et al. (2025b) discovered that fine-tuning GPT-4o on intentionally insecure code causes the model to write insecure code even when responding to unrelated prompts.”</em></p> </blockquote> <p>The research identifies a critical issue with existing alignment methods: when models are fine-tuned on specific tasks, they may exhibit similar behavioral patterns in completely unrelated tasks. For example, if a model is fine-tuned on insecure code, it may tend to write insecure code even when responding to completely unrelated prompts.</p> <p>This phenomenon is termed “emergent misalignment,” indicating that models learn not just specific task skills, but also underlying behavioral tendencies or “persona features.”</p> <h2 id="solution">Solution</h2> <h3 id="core-concept-persona-features-control">Core Concept: Persona Features Control</h3> <p>The solution proposed in this paper centers on: <strong>Identifying and controlling internal “persona features” within models to manage emergent misalignment issues</strong>.</p> <blockquote> <p><em>“We extend this work, demonstrating emergent misalignment across diverse scenarios, including reinforcement learning on reasoning models, fine-tuning on various synthetic datasets, and in models without safety training.”</em></p> </blockquote> <p>The research team expanded the scope of this problem to multiple different scenarios:</p> <ol> <li><strong>Reinforcement learning on reasoning models</strong></li> <li><strong>Fine-tuning on various synthetic datasets</strong></li> <li><strong>Models without safety training</strong></li> </ol> <h3 id="methodology-representation-analysis-and-control">Methodology: Representation Analysis and Control</h3> <blockquote> <p><em>“To investigate the mechanisms behind this generalized misalignment, we introduce a novel approach that analyzes model representations before and after fine-tuning.”</em></p> </blockquote> <p>The research team proposed a novel method to investigate the mechanisms behind this generalized misalignment:</p> <ol> <li><strong>Representation Analysis</strong>: Analyzing changes in model internal representations before and after fine-tuning</li> <li><strong>Persona Feature Identification</strong>: Identifying “persona features” related to specific behavioral patterns</li> <li><strong>Feature Control</strong>: Developing methods to control these features to manage model behavior</li> </ol> <h3 id="toxic-persona-features">Toxic Persona Features</h3> <blockquote> <p><em>“This approach reveals several ‘misaligned persona’ features in activation space, including a toxic persona feature which most strongly controls emergent misalignment and can be used to protect whether a model will exhibit such behavior.”</em></p> </blockquote> <p>The research discovered several key “misaligned persona” features, with the most important being:</p> <ul> <li><strong>Toxic Persona Features</strong>: Features that most strongly control emergent misalignment</li> <li><strong>Predictive Capability</strong>: Can be used to predict whether a model will exhibit misaligned behavior</li> <li><strong>Protection Mechanism</strong>: Can be used to protect models from misalignment effects</li> </ul> <h2 id="limitation">Limitation</h2> <h3 id="potential-research-limitations">Potential Research Limitations</h3> <ol> <li> <p><strong>Scope Limitations</strong>: The research primarily focuses on specific types of misalignment problems and may not cover all possible misalignment scenarios</p> </li> <li> <p><strong>Model Dependency</strong>: The discovered persona features may be related to specific model architectures or training methods, with generalizability yet to be verified</p> </li> <li> <p><strong>Control Precision</strong>: While persona features can be identified and controlled, the precision and stability of control still require further research</p> </li> <li> <p><strong>Computational Cost</strong>: Representation analysis and feature control may require additional computational resources</p> </li> </ol> <h2 id="experiment">Experiment</h2> <h3 id="experimental-design-and-datasets">Experimental Design and Datasets</h3> <p>The research team conducted experiments across multiple different scenarios:</p> <ol> <li><strong>Reinforcement Learning Scenarios</strong>: Reinforcement learning training on reasoning models</li> <li><strong>Fine-tuning Scenarios</strong>: Fine-tuning using various synthetic datasets</li> <li><strong>No Safety Training Scenarios</strong>: Testing on models without safety training</li> </ol> <h3 id="experimental-results">Experimental Results</h3> <blockquote> <p><em>“Additionally, we investigate mitigation strategies and find that adding a few hundred benign samples efficiently restores alignment.”</em></p> </blockquote> <p>The experimental results show:</p> <ol> <li><strong>Emergent misalignment does exist</strong>: This phenomenon was observed across multiple different scenarios</li> <li><strong>Persona features can be identified</strong>: Successfully identified features related to misalignment</li> <li><strong>Control methods are effective</strong>: Misalignment can be effectively managed by controlling persona features</li> <li><strong>Mitigation strategies are effective</strong>: Adding a small number of benign samples can effectively restore alignment</li> </ol> <h2 id="observation">Observation</h2> <h3 id="interesting-findings">Interesting Findings</h3> <ol> <li> <p><strong>Power of Few Samples</strong>: The research found that adding just a few hundred benign samples can effectively restore model alignment, suggesting that misalignment problems may be easier to solve than expected.</p> </li> <li> <p><strong>Cross-task Generalization</strong>: Misalignment not only appears in related tasks but also generalizes to completely unrelated tasks, revealing deep mechanisms of model learning.</p> </li> <li> <p><strong>Structure of Representation Space</strong>: Clear “persona feature” structures exist in the model’s activation space, providing new perspectives for understanding model behavior.</p> </li> </ol> <h2 id="insights">Insights</h2> <p>This paper brings several important insights to the AI safety field:</p> <p><strong>Theoretical Contribution</strong>: This is the first systematic study of emergent misalignment phenomena, proposing an explanatory framework based on persona features. This provides a new theoretical foundation for understanding language model generalization behavior.</p> <p><strong>Methodological Innovation</strong>: The proposed representation analysis method provides a powerful tool for studying model internal mechanisms. By analyzing features in activation space, we can better understand how models learn and generalize behavioral patterns.</p> <p><strong>Practical Value</strong>: The finding that a small number of benign samples can restore alignment has important practical value. This suggests that in actual deployment, we may not need to completely retrain models to solve misalignment problems.</p> <p><strong>Safety Significance</strong>: This research reveals an important safety risk: models may exhibit misaligned behavior in unexpected situations. At the same time, it provides methods for detecting and mitigating such risks.</p> <p><strong>Future Directions</strong>: This work opens several interesting research directions, including deeper understanding of persona feature formation mechanisms, developing more precise control methods, and exploring the applicability of these findings in other types of AI systems.</p> <p>From a broader perspective, this research emphasizes the importance of understanding and controlling model behavior in an era where AI systems are becoming increasingly powerful and autonomous. It reminds us that AI alignment is not just a training-time problem, but a challenge that requires continuous attention throughout the entire model lifecycle.</p> <hr/> <h2 id="concise-summary">Concise Summary</h2> <p>• <strong>Core Problem</strong>: Language models exhibit “emergent misalignment” phenomena, where fine-tuning on specific tasks leads to similar behavioral tendencies in unrelated tasks</p> <p>• <strong>Key Finding</strong>: Models contain identifiable “persona features,” particularly “toxic persona features,” that control the emergence of misaligned behaviors</p> <p>• <strong>Solution</strong>: By analyzing model representation space, these persona features can be identified and controlled to manage misalignment issues</p> <p>• <strong>Practical Value</strong>: Adding just a few hundred benign samples can effectively restore model alignment, providing feasible solutions for actual deployment</p> <p>• <strong>Safety Significance</strong>: Reveals an important but previously under-recognized safety risk in AI systems while providing detection and mitigation methods</p> <p>• <strong>Innovation</strong>: First introduction of “persona features” concept into AI alignment research, providing a new theoretical framework for understanding model behavior</p> <p>• <strong>Future Impact</strong>: This research opens new research directions in AI safety, particularly significant for understanding and controlling model generalization behavior</p>]]></content><author><name></name></author><category term="paper"/><category term="english"/><category term="ai-safety"/><category term="alignment"/><category term="persona-features"/><category term="misalignment"/><summary type="html"><![CDATA[OpenAI research team explores how language models generalize behaviors from training to broader deployment distributions, focusing on emergent misalignment issues. The study reveals that controlling persona features can effectively manage model misalignment behaviors, providing important insights for AI safety.]]></summary></entry><entry xml:lang="zh"><title type="html">[中文版] The Illusion of the Illusion of Thinking: 當AI評估方法成為能力判斷的陷阱</title><link href="https://codepydog.github.io/blog/2025/illusion-illusion-thinking-chs/" rel="alternate" type="text/html" title="[中文版] The Illusion of the Illusion of Thinking: 當AI評估方法成為能力判斷的陷阱"/><published>2025-06-16T00:00:00+00:00</published><updated>2025-06-16T00:00:00+00:00</updated><id>https://codepydog.github.io/blog/2025/illusion-illusion-thinking-chs</id><content type="html" xml:base="https://codepydog.github.io/blog/2025/illusion-illusion-thinking-chs/"><![CDATA[<div class="language-switcher" style="text-align: right; margin-bottom: 20px; padding: 8px 0; border-bottom: 1px solid #333;"> <span style="font-size: 14px; color: #888; margin-right: 8px;">🌐 Language:</span> <strong style="color: #007bff; font-weight: 600;">中文</strong> <span style="color: #666; margin: 0 6px;">|</span> <a href="/blog/2025/illusion-illusion-thinking-en/" style="color: #007bff; text-decoration: none; font-weight: 500; transition: opacity 0.2s;" onmouseover="this.style.opacity='0.7'" onmouseout="this.style.opacity='1'">English</a> </div> <blockquote> <p><strong>來源：</strong> <a href="https://arxiv.org/pdf/2506.09250">Paper</a></p> </blockquote> <p>這是一篇由 A. Lawson 撰寫的重要評論文章，全名為 <strong><a href="https://arxiv.org/pdf/2506.09250">The Illusion of the Illusion of Thinking: A Comment on Shojaee et al. (2025)</a></strong>。這篇論文是針對 Shojaee et al. (2025) 關於大型推理模型限制研究的回應，而 Shojaee et al. 的研究正是我們之前分析過的 Apple 論文的後續討論。如果對這個背景感興趣，可以參考之前的筆記：<a href="/blog/2025/illusion-thinking-chs/">The Illusion of Thinking: Apple論文解析</a>。</p> <p>本評論文章提出了一個深刻的觀點：<strong>我們經常將評估方法的限制誤認為是AI系統能力的限制</strong>。</p> <h2 id="核心問題評估方法的陷阱">核心問題：評估方法的陷阱</h2> <p>Shojaee et al. (2025) 聲稱發現了大型推理模型的基本限制，通過系統性評估規劃謎題，發現模型準確度在超過某些複雜度閾值後會”崩潰”至零。</p> <blockquote> <p><em>“Shojaee et al. (2025) claim to have identified fundamental limitations in Large Reasoning Models through systematic evaluation on planning puzzles. Their central finding—that model accuracy ‘collapses’ to zero beyond certain complexity thresholds—has significant implications for AI reasoning research.”</em></p> </blockquote> <p>然而，本評論文章指出這些表面上的失敗實際上源於實驗設計的限制，而非模型本身的推理能力缺陷。</p> <blockquote> <p><em>“However, our analysis reveals that these apparent failures stem from experimental design limitations rather than fundamental reasoning failures.”</em></p> </blockquote> <h2 id="三大關鍵問題">三大關鍵問題</h2> <h3 id="問題一模型主動識別輸出限制">問題一：模型主動識別輸出限制</h3> <p><strong>最重要的發現：</strong> 模型實際上能夠主動識別何時接近輸出限制，並做出合理的停止決策。</p> <p>論文引用了 @scaling01 在 Twitter 上的複製實驗，捕捉到模型輸出明確表示：</p> <blockquote> <p><em>“The pattern continues, but to avoid making this too long, I’ll stop here”</em></p> </blockquote> <p>這證明模型理解解決方案的模式，但選擇因實際約束而截斷輸出。</p> <blockquote> <p><em>“This demonstrates that models understand the solution pattern but choose to truncate output due to practical constraints.”</em></p> </blockquote> <p><strong>關鍵洞察：</strong> 將模型的這種行為誤判為”推理崩潰”反映了自動評估系統的根本問題，無法區分模型的意識和決策能力。</p> <h3 id="問題二token限制的數學分析">問題二：Token限制的數學分析</h3> <p>論文提供了詳細的數學分析，說明為什麼河內塔問題會遇到token限制：</p> <p><strong>線性增長模式（僅輸出最終序列）：</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>T_final(N) ≈ 10(2^N - 1) + C
</code></pre></div></div> <p><strong>最大可解決規模：</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>N_max ≈ log₂(L_max/10)
</code></pre></div></div> <p>對於不同的token限制：</p> <ul> <li>L_max = 64,000：N_max ≈ 12-13</li> <li>L_max = 100,000：N_max ≈ 13</li> </ul> <p><strong>關鍵發現：</strong> 報告的”崩潰”發生在 N = 9 之前，遠早於這些理論限制。</p> <blockquote> <p><em>“Interestingly, the reported ‘collapse’ before N = 9 for most models occurs well before these theoretical limits. This suggests that models are making a decision to terminate output before actually reaching their context window limits.”</em></p> </blockquote> <h3 id="問題三不可能謎題的陷阱">問題三：不可能謎題的陷阱</h3> <p>在河流渡河實驗中，評估問題被戲劇性地放大：</p> <blockquote> <p><em>“The evaluation issues are compounded dramatically in the River Crossing experiments. Shojaee et al. test instances with N ≥ 6 actors/agents using boat capacity b = 3. However, it is a well-established result that the Missionaries-Cannibals puzzle (and its variants) has no solution for N &gt; 5 when b = 3.”</em></p> </blockquote> <p><strong>問題本質：</strong> 評估框架包含了數學上不可能解決的問題，然後將模型無法解決這些問題歸咎於推理能力不足。</p> <h2 id="實驗驗證替代表示法恢復性能">實驗驗證：替代表示法恢復性能</h2> <p>為了測試失敗是否反映推理限制或格式約束，作者進行了初步測試：</p> <p><strong>測試提示：</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"Solve Tower of Hanoi with 15 disks. Output a Lua function that prints the solution when called."
</code></pre></div></div> <p><strong>驚人結果：</strong></p> <ul> <li>多個先進模型（Claude-3.7-Sonnet, Claude Opus 4, OpenAI o3, Google Gemini 2.5）獲得了非常高的準確率</li> <li>在5,000個tokens內完成</li> <li>生成的解決方案正確實現了遞歸算法</li> </ul> <blockquote> <p><em>“The generated solutions correctly implement the recursive algorithm, demonstrating intact reasoning capabilities when freed from exhaustive enumeration requirements.”</em></p> </blockquote> <h2 id="複雜度聲稱的重新評估">複雜度聲稱的重新評估</h2> <p>論文指出原始研究使用”組合深度”（最小移動數）作為複雜度指標的問題：</p> <table> <thead> <tr> <th>謎題</th> <th>解決方案長度</th> <th>分支因子</th> <th>計算複雜度</th> </tr> </thead> <tbody> <tr> <td>河內塔</td> <td>2^N - 1</td> <td>1</td> <td>O(1) 每步</td> </tr> <tr> <td>積木世界</td> <td>O(N)</td> <td>O(N²)</td> <td>線性（近最優）/ NP-hard（最優）</td> </tr> </tbody> </table> <p><strong>關鍵洞察：</strong> 河內塔儘管需要指數級的移動數，但每步的決策過程是微不足道的 O(1)。積木世界則困難得多。</p> <h3 id="最優化問題的區別">最優化問題的區別</h3> <p>積木世界的提示明確要求最優化：</p> <blockquote> <p><em>“Find the minimum sequence of moves to transform the initial state into the goal state.”</em></p> </blockquote> <p>雖然解決方案檢查器只驗證正確性而非最優性，但模型會嘗試按照指示尋找最優解，這增加了計算負擔。</p> <h2 id="深層思考評估設計的重要性">深層思考：評估設計的重要性</h2> <h3 id="工程決策-vs-推理失敗">工程決策 vs 推理失敗</h3> <p>論文最創新的洞察是區分了”工程決策”和”推理失敗”：</p> <blockquote> <p><em>“This distinction further emphasizes the importance of evaluation design. Scoring models as ‘failures’ for making reasonable engineering decisions about output length mischaracterizes their actual capabilities.”</em></p> </blockquote> <p><strong>核心觀點：</strong> 當模型選擇不輸出冗長的中間步驟時，這可能是合理的效率考量，而非能力缺陷。</p> <h3 id="模型的自我認知能力">模型的自我認知能力</h3> <p>研究發現模型具有某種自我校準能力：</p> <blockquote> <p><em>“This behavior indicates that models may be poorly calibrated about their own context length capabilities, choosing to stop prematurely.”</em></p> </blockquote> <p><strong>重要發現：</strong> 模型展現出對自身能力的某種認知，會根據情況做出停止決策。</p> <h2 id="對ai評估的深遠影響">對AI評估的深遠影響</h2> <p>這項研究提醒我們需要重新思考AI評估的方法學：</p> <p><strong>未來工作應該：</strong></p> <ol> <li><strong>設計能區分推理能力和輸出約束的評估</strong></li> <li><strong>在評估模型性能前驗證謎題的可解性</strong></li> <li><strong>使用反映計算難度而非僅僅解決方案長度的複雜度指標</strong></li> <li><strong>考慮多種解決方案表示法，以分離算法理解和執行</strong></li> </ol> <h3 id="測量問題的哲學">測量問題的哲學</h3> <p>這讓我聯想到測量問題的哲學：我們的測量工具和方法本身會影響我們對被測量對象的理解。</p> <p><strong>核心啟示：</strong> 在AI評估中，我們需要更加謹慎地設計評估框架，避免將方法論的限制誤認為是系統能力的邊界。</p> <h2 id="與人類認知的類比">與人類認知的類比</h2> <h3 id="效率優先的智能行為">效率優先的智能行為</h3> <p>這項研究讓我想到人類在面對複雜問題時的認知模式：</p> <ul> <li><strong>實用主義</strong> - 人類也會採用啟發式方法，而非暴力枚舉</li> <li><strong>認知負荷管理</strong> - 人類會根據問題複雜度調整思考策略</li> <li><strong>自我監控</strong> - 人類會意識到自己的認知限制並相應調整</li> </ul> <p><strong>深層思考：</strong> AI模型的這種行為是否反映了某種智能的特徵？</p> <h2 id="批判性思考">批判性思考</h2> <h3 id="平衡的重要性">平衡的重要性</h3> <p>雖然這篇評論文章提出了有價值的觀點，但我們也需要注意：</p> <p><strong>避免極端化：</strong> 不要因為批評原始研究就完全忽視AI系統的真實限制</p> <p><strong>保持客觀：</strong> 需要建立更加平衡和細緻的評估框架，既能識別真正的能力邊界，又不會因為方法論問題而產生誤判</p> <h2 id="結論">結論</h2> <p>這篇論文最終提醒我們一個重要道理：</p> <blockquote> <p><em>“The question isn’t whether LRMs can reason, but whether our evaluations can distinguish reasoning from typing.”</em></p> </blockquote> <p><strong>核心啟示：</strong></p> <ul> <li>評估方法的設計會直接影響我們對AI能力的判斷</li> <li>需要區分真正的能力限制和評估框架的限制</li> <li>AI系統的”失敗”可能實際上是合理的工程決策</li> <li>模型展現出的自我認知和適應能力值得重視</li> </ul> <p><strong>在AI能力評估中，”如何問問題”往往和”問什麼問題”一樣重要。</strong></p> <p>這項研究不僅為當前的AI評估提供了重要反思，也為未來構建更可靠的AI能力評估體系指明了方向。我們需要透過”評估的幻象”，看到AI系統的真實能力。</p> <hr/> <h2 id="參考文獻">參考文獻</h2> <ul> <li><a href="https://arxiv.org/pdf/2506.09250">The Illusion of the Illusion of Thinking: A Comment on Shojaee et al. (2025)</a></li> <li><a href="https://arxiv.org/pdf/2506.06941">The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</a></li> </ul>]]></content><author><name></name></author><category term="paper"/><category term="chinese"/><category term="ai-evaluation"/><category term="reasoning"/><category term="model-limitations"/><category term="methodology"/><summary type="html"><![CDATA[這篇評論文章揭露了一個驚人真相：我們經常將AI評估方法的限制誤認為是AI系統能力的限制！研究發現，許多被認為是AI推理失敗的案例，實際上是評估框架設計不當造成的誤判。]]></summary></entry><entry xml:lang="en"><title type="html">The Illusion of the Illusion of Thinking: When AI Evaluation Methods Become Traps for Capability Assessment</title><link href="https://codepydog.github.io/blog/2025/illusion-illusion-thinking-en/" rel="alternate" type="text/html" title="The Illusion of the Illusion of Thinking: When AI Evaluation Methods Become Traps for Capability Assessment"/><published>2025-06-16T00:00:00+00:00</published><updated>2025-06-16T00:00:00+00:00</updated><id>https://codepydog.github.io/blog/2025/illusion-illusion-thinking-en</id><content type="html" xml:base="https://codepydog.github.io/blog/2025/illusion-illusion-thinking-en/"><![CDATA[<div class="language-switcher" style="text-align: right; margin-bottom: 20px; padding: 8px 0; border-bottom: 1px solid #333;"> <span style="font-size: 14px; color: #888; margin-right: 8px;">🌐 Language:</span> <a href="/blog/2025/illusion-illusion-thinking-chs/" style="color: #007bff; text-decoration: none; font-weight: 500; transition: opacity 0.2s;" onmouseover="this.style.opacity='0.7'" onmouseout="this.style.opacity='1'">中文</a> <span style="color: #666; margin: 0 6px;">|</span> <strong style="color: #007bff; font-weight: 600;">English</strong> </div> <blockquote> <p><strong>Source:</strong> <a href="https://arxiv.org/pdf/2506.09250">Paper</a></p> </blockquote> <p>This is an important commentary paper by A. Lawson, officially titled <strong><a href="https://arxiv.org/pdf/2506.09250">The Illusion of the Illusion of Thinking: A Comment on Shojaee et al. (2025)</a></strong>. This paper responds to Shojaee et al. (2025)’s research on Large Reasoning Model limitations, which builds upon the Apple paper we previously analyzed. If you’re interested in this background, you can refer to our previous notes: <a href="/blog/2025/illusion-thinking-en/">The Illusion of Thinking: Apple Paper Analysis</a>.</p> <p>This commentary presents a profound perspective: <strong>we often mistake the limitations of evaluation methods for the limitations of AI systems themselves</strong>.</p> <h2 id="core-problem-the-trap-of-evaluation-methods">Core Problem: The Trap of Evaluation Methods</h2> <p>Shojaee et al. (2025) claimed to have discovered fundamental limitations of Large Reasoning Models through systematic evaluation of planning puzzles, finding that model accuracy “collapses” to zero beyond certain complexity thresholds.</p> <blockquote> <p><em>“Shojaee et al. (2025) claim to have identified fundamental limitations in Large Reasoning Models through systematic evaluation on planning puzzles. Their central finding—that model accuracy ‘collapses’ to zero beyond certain complexity thresholds—has significant implications for AI reasoning research.”</em></p> </blockquote> <p>However, this commentary paper points out that these apparent failures actually stem from experimental design limitations rather than fundamental reasoning capability defects of the models themselves.</p> <blockquote> <p><em>“However, our analysis reveals that these apparent failures stem from experimental design limitations rather than fundamental reasoning failures.”</em></p> </blockquote> <h2 id="three-key-problems">Three Key Problems</h2> <h3 id="problem-one-models-actively-recognize-output-constraints">Problem One: Models Actively Recognize Output Constraints</h3> <p><strong>Most Important Finding:</strong> Models can actually actively recognize when they approach output limits and make reasonable stopping decisions.</p> <p>The paper cites a replication experiment by @scaling01 on Twitter, which captured model outputs explicitly stating:</p> <blockquote> <p><em>“The pattern continues, but to avoid making this too long, I’ll stop here”</em></p> </blockquote> <p>This proves that models understand the solution pattern but choose to truncate output due to practical constraints.</p> <blockquote> <p><em>“This demonstrates that models understand the solution pattern but choose to truncate output due to practical constraints.”</em></p> </blockquote> <p><strong>Key Insight:</strong> Misjudging this model behavior as “reasoning collapse” reflects a fundamental problem with automated evaluation systems that fail to account for model awareness and decision-making capabilities.</p> <h3 id="problem-two-mathematical-analysis-of-token-limitations">Problem Two: Mathematical Analysis of Token Limitations</h3> <p>The paper provides detailed mathematical analysis explaining why Tower of Hanoi problems encounter token limitations:</p> <p><strong>Linear Growth Pattern (outputting only final sequence):</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>T_final(N) ≈ 10(2^N - 1) + C
</code></pre></div></div> <p><strong>Maximum Solvable Scale:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>N_max ≈ log₂(L_max/10)
</code></pre></div></div> <p>For different token limits:</p> <ul> <li>L_max = 64,000: N_max ≈ 12-13</li> <li>L_max = 100,000: N_max ≈ 13</li> </ul> <p><strong>Key Finding:</strong> The reported “collapse” occurs before N = 9, well before these theoretical limits.</p> <blockquote> <p><em>“Interestingly, the reported ‘collapse’ before N = 9 for most models occurs well before these theoretical limits. This suggests that models are making a decision to terminate output before actually reaching their context window limits.”</em></p> </blockquote> <h3 id="problem-three-the-trap-of-impossible-puzzles">Problem Three: The Trap of Impossible Puzzles</h3> <p>In River Crossing experiments, evaluation issues are dramatically compounded:</p> <blockquote> <p><em>“The evaluation issues are compounded dramatically in the River Crossing experiments. Shojaee et al. test instances with N ≥ 6 actors/agents using boat capacity b = 3. However, it is a well-established result that the Missionaries-Cannibals puzzle (and its variants) has no solution for N &gt; 5 when b = 3.”</em></p> </blockquote> <p><strong>Problem Essence:</strong> The evaluation framework includes mathematically impossible problems, then attributes models’ inability to solve these problems to insufficient reasoning capabilities.</p> <h2 id="experimental-validation-alternative-representations-restore-performance">Experimental Validation: Alternative Representations Restore Performance</h2> <p>To test whether failures reflect reasoning limitations or format constraints, the author conducted preliminary testing:</p> <p><strong>Test Prompt:</strong></p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>"Solve Tower of Hanoi with 15 disks. Output a Lua function that prints the solution when called."
</code></pre></div></div> <p><strong>Surprising Results:</strong></p> <ul> <li>Multiple advanced models (Claude-3.7-Sonnet, Claude Opus 4, OpenAI o3, Google Gemini 2.5) achieved very high accuracy</li> <li>Completed within 5,000 tokens</li> <li>Generated solutions correctly implemented recursive algorithms</li> </ul> <blockquote> <p><em>“The generated solutions correctly implement the recursive algorithm, demonstrating intact reasoning capabilities when freed from exhaustive enumeration requirements.”</em></p> </blockquote> <h2 id="reevaluating-complexity-claims">Reevaluating Complexity Claims</h2> <p>The paper points out problems with the original research using “compositional depth” (minimum moves) as a complexity metric:</p> <table> <thead> <tr> <th>Puzzle</th> <th>Solution Length</th> <th>Branching Factor</th> <th>Computational Complexity</th> </tr> </thead> <tbody> <tr> <td>Tower of Hanoi</td> <td>2^N - 1</td> <td>1</td> <td>O(1) per move</td> </tr> <tr> <td>Blocks World</td> <td>O(N)</td> <td>O(N²)</td> <td>Linear (near-optimal) / NP-hard (optimal)</td> </tr> </tbody> </table> <p><strong>Key Insight:</strong> Tower of Hanoi, despite requiring exponentially many moves, has a trivial O(1) decision process per move. Blocks World is much harder.</p> <h3 id="the-optimality-question">The Optimality Question</h3> <p>Blocks World prompts explicitly require optimization:</p> <blockquote> <p><em>“Find the minimum sequence of moves to transform the initial state into the goal state.”</em></p> </blockquote> <p>While the solution checker only verifies correctness rather than optimality, models attempt to follow instructions to find optimal solutions, increasing computational burden.</p> <h2 id="deep-thinking-the-importance-of-evaluation-design">Deep Thinking: The Importance of Evaluation Design</h2> <h3 id="engineering-decisions-vs-reasoning-failures">Engineering Decisions vs Reasoning Failures</h3> <p>The paper’s most innovative insight is distinguishing between “engineering decisions” and “reasoning failures”:</p> <blockquote> <p><em>“This distinction further emphasizes the importance of evaluation design. Scoring models as ‘failures’ for making reasonable engineering decisions about output length mischaracterizes their actual capabilities.”</em></p> </blockquote> <p><strong>Core Viewpoint:</strong> When models choose not to output lengthy intermediate steps, this may be reasonable efficiency consideration, not capability defect.</p> <h3 id="models-self-awareness-capabilities">Models’ Self-Awareness Capabilities</h3> <p>Research found models possess some form of self-calibration ability:</p> <blockquote> <p><em>“This behavior indicates that models may be poorly calibrated about their own context length capabilities, choosing to stop prematurely.”</em></p> </blockquote> <p><strong>Important Finding:</strong> Models demonstrate some form of self-awareness about their capabilities, making stopping decisions based on circumstances.</p> <h2 id="profound-impact-on-ai-evaluation">Profound Impact on AI Evaluation</h2> <p>This research reminds us to rethink AI evaluation methodology:</p> <p><strong>Future work should:</strong></p> <ol> <li><strong>Design evaluations that distinguish between reasoning capability and output constraints</strong></li> <li><strong>Verify puzzle solvability before evaluating model performance</strong></li> <li><strong>Use complexity metrics that reflect computational difficulty, not just solution length</strong></li> <li><strong>Consider multiple solution representations to separate algorithmic understanding from execution</strong></li> </ol> <h3 id="philosophy-of-measurement-problems">Philosophy of Measurement Problems</h3> <p>This reminds me of the philosophy of measurement problems: our measurement tools and methods themselves influence our understanding of the measured objects.</p> <p><strong>Core Insight:</strong> In AI evaluation, we need to more carefully design evaluation frameworks to avoid mistaking methodological limitations for system capability boundaries.</p> <h2 id="analogy-with-human-cognition">Analogy with Human Cognition</h2> <h3 id="efficiency-priority-intelligent-behavior">Efficiency-Priority Intelligent Behavior</h3> <p>This research reminds me of human cognitive patterns when facing complex problems:</p> <ul> <li><strong>Pragmatism</strong> - Humans also use heuristic methods rather than brute force enumeration</li> <li><strong>Cognitive Load Management</strong> - Humans adjust thinking strategies based on problem complexity</li> <li><strong>Self-Monitoring</strong> - Humans are aware of their cognitive limitations and adjust accordingly</li> </ul> <p><strong>Deep Thinking:</strong> Does this behavior of AI models reflect some characteristic of intelligence?</p> <h2 id="critical-thinking">Critical Thinking</h2> <h3 id="the-importance-of-balance">The Importance of Balance</h3> <p>While this commentary paper presents valuable viewpoints, we also need to note:</p> <p><strong>Avoid Extremism:</strong> Don’t completely ignore real limitations of AI systems because of criticizing original research</p> <p><strong>Maintain Objectivity:</strong> Need to establish more balanced and nuanced evaluation frameworks that can identify true capability boundaries without misjudging due to methodological issues</p> <h2 id="conclusion">Conclusion</h2> <p>This paper ultimately reminds us of an important truth:</p> <blockquote> <p><em>“The question isn’t whether LRMs can reason, but whether our evaluations can distinguish reasoning from typing.”</em></p> </blockquote> <p><strong>Core Insights:</strong></p> <ul> <li>Evaluation method design directly affects our judgment of AI capabilities</li> <li>Need to distinguish between true capability limitations and evaluation framework limitations</li> <li>AI systems’ “failures” may actually be reasonable engineering decisions</li> <li>Models’ demonstrated self-awareness and adaptive capabilities deserve attention</li> </ul> <p><strong>In AI capability evaluation, “how to ask questions” is often as important as “what questions to ask.”</strong></p> <p>This research not only provides important reflection for current AI evaluation but also points the way for building more reliable AI capability evaluation systems in the future. We need to see through the “illusion of evaluation” to understand the true capabilities of AI systems.</p> <hr/> <h2 id="references">References</h2> <ul> <li><a href="https://arxiv.org/pdf/2506.09250">The Illusion of the Illusion of Thinking: A Comment on Shojaee et al. (2025)</a></li> <li><a href="https://arxiv.org/pdf/2506.06941">The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</a></li> </ul>]]></content><author><name></name></author><category term="paper"/><category term="english"/><category term="ai-evaluation"/><category term="reasoning"/><category term="model-limitations"/><category term="methodology"/><summary type="html"><![CDATA[This commentary paper reveals a shocking truth: we often mistake the limitations of AI evaluation methods for the limitations of AI systems themselves! Research shows that many cases considered AI reasoning failures are actually misjudgments caused by poorly designed evaluation frameworks.]]></summary></entry><entry xml:lang="zh"><title type="html">[中文版] The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</title><link href="https://codepydog.github.io/blog/2025/illusion-thinking-chs/" rel="alternate" type="text/html" title="[中文版] The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity"/><published>2025-06-07T00:00:00+00:00</published><updated>2025-06-07T00:00:00+00:00</updated><id>https://codepydog.github.io/blog/2025/illusion-thinking-chs</id><content type="html" xml:base="https://codepydog.github.io/blog/2025/illusion-thinking-chs/"><![CDATA[<div class="language-switcher" style="text-align: right; margin-bottom: 20px; padding: 8px 0; border-bottom: 1px solid #333;"> <span style="font-size: 14px; color: #888; margin-right: 8px;">🌐 Language:</span> <strong style="color: #007bff; font-weight: 600;">中文</strong> <span style="color: #666; margin: 0 6px;">|</span> <a href="/blog/2025/illusion-thinking-en/" style="color: #007bff; text-decoration: none; font-weight: 500; transition: opacity 0.2s;" onmouseover="this.style.opacity='0.7'" onmouseout="this.style.opacity='1'">English</a> </div> <blockquote> <p><strong>來源：</strong> <a href="https://arxiv.org/pdf/2506.06941">Paper</a></p> </blockquote> <p>這是一篇來自 Apple 的重要論文，全名為 <strong><a href="https://arxiv.org/pdf/2506.06941">The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</a></strong>。這篇論文挑戰了我們對大型推理模型能力的基本假設，提出了一個發人深省的問題：當前的推理模型是否真的在「思考」，還是只是在執行複雜的模式匹配？</p> <h2 id="核心問題推理模型的真實能力是什麼">核心問題：推理模型的真實能力是什麼？</h2> <h3 id="現有評估方法的局限性">現有評估方法的局限性</h3> <p>當前對大型語言模型推理能力的評估主要存在以下問題：</p> <blockquote> <p><em>“Current evaluations primarily focus on established mathematical and coding benchmarks, emphasizing final answer accuracy. However, these approaches fail to provide insights into the reasoning traces’ structure and quality.”</em></p> </blockquote> <p><strong>三大核心問題：</strong></p> <ol> <li><strong>過度關注最終答案準確性</strong>：現有評估主要看結果是否正確，而忽略了推理過程的品質</li> <li><strong>缺乏對推理結構的深入分析</strong>：沒有系統性地分析模型的內部推理軌跡</li> <li><strong>對問題複雜度的理解不足</strong>：缺乏從問題複雜度角度來理解模型性能的框架</li> </ol> <h3 id="大型推理模型的興起與挑戰">大型推理模型的興起與挑戰</h3> <p>論文指出了一個關鍵趨勢：</p> <blockquote> <p><em>“Recent generations of frontier language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood.”</em></p> </blockquote> <p>雖然新一代的推理模型（如 Claude 3.7 Thinking、GPT-o1 等）在benchmark上表現優異，但我們對其真實能力的理解仍然不足。</p> <h2 id="創新解決方案基於複雜度的系統性分析框架">創新解決方案：基於複雜度的系統性分析框架</h2> <h3 id="核心方法論">核心方法論</h3> <p>Apple 研究團隊提出了一個革命性的評估框架：</p> <blockquote> <p><em>“In this work, we systematically investigate these aspects of LRMs by constructing puzzle environments that allow precise manipulation of computational complexity while maintaining consistent logical structures.”</em></p> </blockquote> <p><strong>三個關鍵創新：</strong></p> <ol> <li> <p><strong>可控制的拼圖環境設計</strong></p> <ul> <li>精確控制計算複雜度</li> <li>保持一致的邏輯結構</li> <li>消除外部變數干擾</li> </ul> </li> <li> <p><strong>推理軌跡分析</strong></p> <ul> <li>同時分析最終答案和中間推理過程</li> <li>從初始狀態到目標狀態的完整推理路徑驗證</li> <li>多維度性能評估</li> </ul> </li> <li> <p><strong>三個關鍵性能指標</strong></p> <ul> <li><strong>最終答案準確性</strong></li> <li><strong>推理軌跡品質</strong></li> <li><strong>計算效率</strong></li> </ul> </li> </ol> <h3 id="實驗設計的巧思">實驗設計的巧思</h3> <p>研究團隊設計了一個可控制複雜度的拼圖環境，這種設計的優勢在於：</p> <blockquote> <p><em>“This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into LRMs’ computational behavior.”</em></p> </blockquote> <p>通過調整拼圖的大小和步驟數，研究者可以精確控制問題的計算複雜度，同時保持邏輯結構的一致性。</p> <h2 id="震撼發現反直覺的複雜度擴展邊界">震撼發現：反直覺的複雜度擴展邊界</h2> <h3 id="核心發現">核心發現</h3> <p>論文最重要的發現挑戰了AI領域的一個基本假設：</p> <blockquote> <p><em>“Moreover, they exhibit a counterintuitive scaling limit: their reasoning effect increases with problem complexity up to a point, then declines despite having an adequate token budget.”</em></p> </blockquote> <p><strong>關鍵洞察：</strong></p> <ul> <li>推理效果隨問題複雜度增加而提升，但只到某個臨界點</li> <li>超過臨界點後，即使有充足的token預算，性能仍會下降</li> <li>這種現象揭示了當前推理模型的根本限制</li> </ul> <h3 id="實驗結果分析">實驗結果分析</h3> <p>從論文的實驗結果可以看到三個重要模式：</p> <ol> <li> <p><strong>準確性vs複雜度曲線</strong>：</p> <ul> <li>隨著複雜度增加，模型準確性呈現倒U型曲線</li> <li>存在一個最佳複雜度點，超過後性能急劇下降</li> </ul> </li> <li> <p><strong>Token使用模式</strong>：</p> <ul> <li>模型會根據問題複雜度動態調整思考長度</li> <li>但在某個點後，增加思考長度無法提升性能</li> </ul> </li> <li> <p><strong>推理品質差異</strong>：</p> <ul> <li>正確解決的案例中，模型傾向於早期找到答案</li> <li>失敗案例中，模型經常專注於錯誤方向，浪費計算資源</li> </ul> </li> </ol> <h2 id="深層思考思考的本質">深層思考：「思考」的本質</h2> <h3 id="推理效率的悖論">推理效率的悖論</h3> <p>論文發現了一個有趣的現象：</p> <blockquote> <p><em>“Both cases reveal inefficiencies in the reasoning process.”</em></p> </blockquote> <p><strong>兩種低效模式：</strong></p> <ol> <li><strong>過早收斂</strong>：在簡單問題上可能過度思考</li> <li><strong>錯誤固化</strong>：在複雜問題上容易陷入錯誤思路並難以自我糾正</li> </ol> <h3 id="對agi發展的啟示">對AGI發展的啟示</h3> <p>論文提出了一個深刻的哲學問題：</p> <blockquote> <p><em>“emergence suggests a potential paradigm shift in how LLM systems approach complex reasoning and problem-solving tasks, with some researchers proposing them as significant steps toward more general artificial intelligence capabilities.”</em></p> </blockquote> <p>但同時也保持了理性的態度：</p> <blockquote> <p><em>“Despite these claims and performance advancements, the fundamental benefits and limitations of LRMs remain insufficiently understood.”</em></p> </blockquote> <h2 id="實用價值與未來方向">實用價值與未來方向</h2> <h3 id="對實際應用的指導意義">對實際應用的指導意義</h3> <p>這項研究為AI應用提供了重要的實用指導：</p> <p><strong>1. 成本優化策略</strong></p> <ul> <li>幫助確定最佳的推理資源配置</li> <li>避免在超過最佳複雜度點的任務上浪費資源</li> </ul> <p><strong>2. 性能預期管理</strong></p> <ul> <li>為不同複雜度的任務設定合理的性能預期</li> <li>理解模型能力的邊界</li> </ul> <p><strong>3. 模型選擇指南</strong></p> <ul> <li>為特定應用選擇合適的推理模型</li> <li>平衡性能與成本</li> </ul> <h3 id="未來研究方向">未來研究方向</h3> <p>這項工作開啟了幾個重要的研究方向：</p> <p><strong>1. 適應性推理</strong></p> <ul> <li>如何讓模型根據問題複雜度動態調整推理策略</li> <li>開發複雜度感知的推理算法</li> </ul> <p><strong>2. 推理效率優化</strong></p> <ul> <li>如何在保持準確性的同時提高推理效率</li> <li>設計更智能的計算資源分配機制</li> </ul> <p><strong>3. 評估方法學創新</strong></p> <ul> <li>發展更全面的推理能力評估框架</li> <li>關注過程而非僅僅關注結果</li> </ul> <h2 id="心得">心得</h2> <h3 id="與人類認知的相似性">與人類認知的相似性</h3> <p>這項研究讓我想到人類在解決複雜問題時的認知模式。人類也會經歷類似的「效率邊界」：</p> <ul> <li><strong>過度思考的陷阱</strong>：有時候想太多反而會降低問題解決的效果</li> <li><strong>認知負荷限制</strong>：超過認知容量後，思考品質會下降</li> <li><strong>直覺vs分析</strong>：簡單問題靠直覺，複雜問題需要系統性分析</li> </ul> <p>這種相似性是否暗示當前的推理模型在某種程度上確實捕捉到了認知過程的某些特徵？</p> <h3 id="對快思慢想理論的呼應">對「快思慢想」理論的呼應</h3> <p>這項研究似乎為Daniel Kahneman的「快思慢想」理論在AI中的應用提供了實證支持：</p> <ul> <li><strong>系統1（快思）</strong>：適合處理簡單、熟悉的問題</li> <li><strong>系統2（慢想）</strong>：適合處理複雜、需要深度分析的問題</li> </ul> <p>模型在不同複雜度問題上的表現差異，呼應了人類認知的雙系統理論。</p> <h3 id="對ai安全的思考">對AI安全的思考</h3> <p>這項研究也提醒我們關注AI系統的可靠性：</p> <ul> <li><strong>能力邊界的重要性</strong>：了解模型的限制對於安全部署至關重要</li> <li><strong>過度自信的風險</strong>：模型可能在超出能力範圍的問題上表現出不當的自信</li> <li><strong>可解釋性的必要性</strong>：需要更好地理解模型的推理過程</li> </ul> <h2 id="結論">結論</h2> <p>這篇論文為我們理解大型推理模型提供了全新的視角。通過揭示「推理效果的複雜度邊界」這一反直覺現象，它挑戰了我們對AI能力的基本假設，提醒我們在追求更強大的AI系統時，需要更深入地理解這些系統的工作原理。</p> <p><strong>核心啟示：</strong></p> <ul> <li>更多計算資源並不總是帶來更好的性能</li> <li>推理模型存在根本性的能力邊界</li> <li>我們需要重新思考如何評估和優化推理能力</li> </ul> <p>這項工作不僅為當前的AI研究提供了重要洞察，也為未來構建更可靠、更高效的推理系統指明了方向。正如論文標題所暗示的，我們需要透過「思考的幻象」，看到推理模型的真實本質。</p> <hr/> <h2 id="參考文獻">參考文獻</h2> <ul> <li><a href="https://arxiv.org/pdf/2506.06941">The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</a></li> <li><a href="https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow">Thinking, Fast and Slow - Daniel Kahneman</a></li> </ul>]]></content><author><name></name></author><category term="paper"/><category term="chinese"/><category term="reasoning"/><category term="llm"/><category term="evaluation"/><summary type="html"><![CDATA[這篇Apple論文發現了一個驚人事實：推理模型並非越複雜越好！研究顯示當問題超過某個複雜度後，即使給模型更多時間思考，表現反而會下降。這挑戰了我們對AI推理能力的基本認知。]]></summary></entry><entry xml:lang="en"><title type="html">The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</title><link href="https://codepydog.github.io/blog/2025/illusion-thinking-en/" rel="alternate" type="text/html" title="The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity"/><published>2025-06-07T00:00:00+00:00</published><updated>2025-06-07T00:00:00+00:00</updated><id>https://codepydog.github.io/blog/2025/illusion-thinking-en</id><content type="html" xml:base="https://codepydog.github.io/blog/2025/illusion-thinking-en/"><![CDATA[<div class="language-switcher" style="text-align: right; margin-bottom: 20px; padding: 8px 0; border-bottom: 1px solid #333;"> <span style="font-size: 14px; color: #888; margin-right: 8px;">🌐 Language:</span> <a href="/blog/2025/illusion-thinking-chs/" style="color: #007bff; text-decoration: none; font-weight: 500; transition: opacity 0.2s;" onmouseover="this.style.opacity='0.7'" onmouseout="this.style.opacity='1'">中文</a> <span style="color: #666; margin: 0 6px;">|</span> <strong style="color: #007bff; font-weight: 600;">English</strong> </div> <blockquote> <p><strong>Source:</strong> <a href="https://arxiv.org/pdf/2506.06941">Paper</a></p> </blockquote> <p>This is a groundbreaking paper from Apple, officially titled <strong><a href="https://arxiv.org/pdf/2506.06941">The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</a></strong>. This paper challenges our fundamental assumptions about the capabilities of large reasoning models and poses a thought-provoking question: Are current reasoning models truly “thinking,” or are they merely executing complex pattern matching?</p> <h2 id="core-problem-what-are-the-true-capabilities-of-reasoning-models">Core Problem: What Are the True Capabilities of Reasoning Models?</h2> <h3 id="limitations-of-current-evaluation-methods">Limitations of Current Evaluation Methods</h3> <p>Current evaluations of large language models’ reasoning capabilities primarily suffer from the following issues:</p> <blockquote> <p><em>“Current evaluations primarily focus on established mathematical and coding benchmarks, emphasizing final answer accuracy. However, these approaches fail to provide insights into the reasoning traces’ structure and quality.”</em></p> </blockquote> <p><strong>Three Core Problems:</strong></p> <ol> <li><strong>Over-emphasis on Final Answer Accuracy</strong>: Current evaluations mainly focus on whether results are correct, while ignoring the quality of the reasoning process</li> <li><strong>Lack of Deep Analysis of Reasoning Structure</strong>: No systematic analysis of models’ internal reasoning traces</li> <li><strong>Insufficient Understanding of Problem Complexity</strong>: Lack of framework for understanding model performance from a problem complexity perspective</li> </ol> <h3 id="the-rise-and-challenges-of-large-reasoning-models">The Rise and Challenges of Large Reasoning Models</h3> <p>The paper identifies a key trend:</p> <blockquote> <p><em>“Recent generations of frontier language models have introduced Large Reasoning Models (LRMs) that generate detailed thinking processes before providing answers. While these models demonstrate improved performance on reasoning benchmarks, their fundamental capabilities, scaling properties, and limitations remain insufficiently understood.”</em></p> </blockquote> <p>Although new-generation reasoning models (such as Claude 3.7 Thinking, GPT-o1, etc.) perform excellently on benchmarks, our understanding of their true capabilities remains insufficient.</p> <h2 id="innovative-solution-systematic-analysis-framework-based-on-complexity">Innovative Solution: Systematic Analysis Framework Based on Complexity</h2> <h3 id="core-methodology">Core Methodology</h3> <p>Apple’s research team proposed a revolutionary evaluation framework:</p> <blockquote> <p><em>“In this work, we systematically investigate these aspects of LRMs by constructing puzzle environments that allow precise manipulation of computational complexity while maintaining consistent logical structures.”</em></p> </blockquote> <p><strong>Three Key Innovations:</strong></p> <ol> <li> <p><strong>Controllable Puzzle Environment Design</strong></p> <ul> <li>Precise control of computational complexity</li> <li>Maintaining consistent logical structures</li> <li>Eliminating external variable interference</li> </ul> </li> <li> <p><strong>Reasoning Trace Analysis</strong></p> <ul> <li>Simultaneous analysis of final answers and intermediate reasoning processes</li> <li>Complete reasoning path verification from initial state to target state</li> <li>Multi-dimensional performance evaluation</li> </ul> </li> <li> <p><strong>Three Key Performance Metrics</strong></p> <ul> <li><strong>Final Answer Accuracy</strong></li> <li><strong>Reasoning Trace Quality</strong></li> <li><strong>Computational Efficiency</strong></li> </ul> </li> </ol> <h3 id="ingenious-experimental-design">Ingenious Experimental Design</h3> <p>The research team designed a controllable complexity puzzle environment, with the advantage being:</p> <blockquote> <p><em>“This setup enables the analysis of not only final answers but also the internal reasoning traces, offering insights into LRMs’ computational behavior.”</em></p> </blockquote> <p>By adjusting puzzle size and step count, researchers can precisely control the computational complexity of problems while maintaining consistency in logical structure.</p> <h2 id="shocking-discovery-counterintuitive-complexity-scaling-boundaries">Shocking Discovery: Counterintuitive Complexity Scaling Boundaries</h2> <h3 id="core-finding">Core Finding</h3> <p>The paper’s most important discovery challenges a fundamental assumption in the AI field:</p> <blockquote> <p><em>“Moreover, they exhibit a counterintuitive scaling limit: their reasoning effect increases with problem complexity up to a point, then declines despite having an adequate token budget.”</em></p> </blockquote> <p><strong>Key Insights:</strong></p> <ul> <li>Reasoning effectiveness improves with problem complexity, but only up to a critical point</li> <li>Beyond the critical point, performance still declines even with adequate token budget</li> <li>This phenomenon reveals fundamental limitations of current reasoning models</li> </ul> <h3 id="experimental-results-analysis">Experimental Results Analysis</h3> <p>From the paper’s experimental results, we can see three important patterns:</p> <ol> <li> <p><strong>Accuracy vs Complexity Curve</strong>:</p> <ul> <li>As complexity increases, model accuracy shows an inverted U-shaped curve</li> <li>There exists an optimal complexity point, beyond which performance drops sharply</li> </ul> </li> <li> <p><strong>Token Usage Patterns</strong>:</p> <ul> <li>Models dynamically adjust thinking length based on problem complexity</li> <li>But beyond a certain point, increasing thinking length cannot improve performance</li> </ul> </li> <li> <p><strong>Reasoning Quality Differences</strong>:</p> <ul> <li>In correctly solved cases, models tend to find answers early</li> <li>In failed cases, models often focus on wrong directions, wasting computational resources</li> </ul> </li> </ol> <h2 id="deep-thinking-the-nature-of-thinking">Deep Thinking: The Nature of “Thinking”</h2> <h3 id="the-paradox-of-reasoning-efficiency">The Paradox of Reasoning Efficiency</h3> <p>The paper discovered an interesting phenomenon:</p> <blockquote> <p><em>“Both cases reveal inefficiencies in the reasoning process.”</em></p> </blockquote> <p><strong>Two Types of Inefficient Patterns:</strong></p> <ol> <li><strong>Premature Convergence</strong>: May overthink on simple problems</li> <li><strong>Error Fixation</strong>: Easily trapped in wrong thinking on complex problems and difficult to self-correct</li> </ol> <h3 id="implications-for-agi-development">Implications for AGI Development</h3> <p>The paper poses a profound philosophical question:</p> <blockquote> <p><em>“emergence suggests a potential paradigm shift in how LLM systems approach complex reasoning and problem-solving tasks, with some researchers proposing them as significant steps toward more general artificial intelligence capabilities.”</em></p> </blockquote> <p>But also maintains a rational attitude:</p> <blockquote> <p><em>“Despite these claims and performance advancements, the fundamental benefits and limitations of LRMs remain insufficiently understood.”</em></p> </blockquote> <h2 id="practical-value-and-future-directions">Practical Value and Future Directions</h2> <h3 id="guidance-for-practical-applications">Guidance for Practical Applications</h3> <p>This research provides important practical guidance for AI applications:</p> <p><strong>1. Cost Optimization Strategies</strong></p> <ul> <li>Help determine optimal reasoning resource allocation</li> <li>Avoid wasting resources on tasks beyond the optimal complexity point</li> </ul> <p><strong>2. Performance Expectation Management</strong></p> <ul> <li>Set reasonable performance expectations for tasks of different complexity</li> <li>Understand the boundaries of model capabilities</li> </ul> <p><strong>3. Model Selection Guidelines</strong></p> <ul> <li>Select appropriate reasoning models for specific applications</li> <li>Balance performance and cost</li> </ul> <h3 id="future-research-directions">Future Research Directions</h3> <p>This work opens several important research directions:</p> <p><strong>1. Adaptive Reasoning</strong></p> <ul> <li>How to make models dynamically adjust reasoning strategies based on problem complexity</li> <li>Develop complexity-aware reasoning algorithms</li> </ul> <p><strong>2. Reasoning Efficiency Optimization</strong></p> <ul> <li>How to improve reasoning efficiency while maintaining accuracy</li> <li>Design smarter computational resource allocation mechanisms</li> </ul> <p><strong>3. Evaluation Methodology Innovation</strong></p> <ul> <li>Develop more comprehensive reasoning capability evaluation frameworks</li> <li>Focus on process rather than just results</li> </ul> <h2 id="personal-reflections-and-insights">Personal Reflections and Insights</h2> <h3 id="similarities-with-human-cognition">Similarities with Human Cognition</h3> <p>This research reminds me of human cognitive patterns when solving complex problems. Humans also experience similar “efficiency boundaries”:</p> <ul> <li><strong>The Trap of Overthinking</strong>: Sometimes thinking too much actually reduces problem-solving effectiveness</li> <li><strong>Cognitive Load Limitations</strong>: Thinking quality declines when cognitive capacity is exceeded</li> <li><strong>Intuition vs Analysis</strong>: Simple problems rely on intuition, complex problems need systematic analysis</li> </ul> <p>Does this similarity suggest that current reasoning models have indeed captured some characteristics of cognitive processes to some extent?</p> <h3 id="echoing-thinking-fast-and-slow-theory">Echoing “Thinking, Fast and Slow” Theory</h3> <p>This research seems to provide empirical support for Daniel Kahneman’s “Thinking, Fast and Slow” theory in AI applications:</p> <ul> <li><strong>System 1 (Fast Thinking)</strong>: Suitable for handling simple, familiar problems</li> <li><strong>System 2 (Slow Thinking)</strong>: Suitable for handling complex problems requiring deep analysis</li> </ul> <p>The performance differences of models on problems of varying complexity echo the dual-system theory of human cognition.</p> <h3 id="considerations-for-ai-safety">Considerations for AI Safety</h3> <p>This research also reminds us to pay attention to the reliability of AI systems:</p> <ul> <li><strong>Importance of Capability Boundaries</strong>: Understanding model limitations is crucial for safe deployment</li> <li><strong>Risk of Overconfidence</strong>: Models may show inappropriate confidence on problems beyond their capability range</li> <li><strong>Necessity of Explainability</strong>: Need better understanding of models’ reasoning processes</li> </ul> <h2 id="conclusion">Conclusion</h2> <p>This paper provides a completely new perspective for understanding large reasoning models. By revealing the counterintuitive phenomenon of “reasoning effectiveness complexity boundaries,” it challenges our basic assumptions about AI capabilities and reminds us that when pursuing more powerful AI systems, we need to understand these systems’ working principles more deeply.</p> <p><strong>Core Insights:</strong></p> <ul> <li>More computational resources don’t always lead to better performance</li> <li>Reasoning models have fundamental capability boundaries</li> <li>We need to rethink how to evaluate and optimize reasoning capabilities</li> </ul> <p>This work not only provides important insights for current AI research but also points the way for building more reliable and efficient reasoning systems in the future. As the paper’s title suggests, we need to see through the “illusion of thinking” to understand the true nature of reasoning models.</p> <hr/> <h2 id="references">References</h2> <ul> <li><a href="https://arxiv.org/pdf/2506.06941">The Illusion of Thinking: Understanding the Strengths and Limitations of Reasoning Models via the Lens of Problem Complexity</a></li> <li><a href="https://en.wikipedia.org/wiki/Thinking,_Fast_and_Slow">Thinking, Fast and Slow - Daniel Kahneman</a></li> </ul>]]></content><author><name></name></author><category term="paper"/><category term="english"/><category term="reasoning"/><category term="llm"/><category term="evaluation"/><summary type="html"><![CDATA[This Apple paper reveals a shocking truth: reasoning models aren't always better with more complexity! Research shows that when problems exceed a certain complexity threshold, performance actually declines even with more thinking time. This challenges our basic understanding of AI reasoning capabilities.]]></summary></entry><entry><title type="html">a post with plotly.js</title><link href="https://codepydog.github.io/blog/2025/plotly/" rel="alternate" type="text/html" title="a post with plotly.js"/><published>2025-03-26T14:24:00+00:00</published><updated>2025-03-26T14:24:00+00:00</updated><id>https://codepydog.github.io/blog/2025/plotly</id><content type="html" xml:base="https://codepydog.github.io/blog/2025/plotly/"><![CDATA[<p>This is an example post with some <a href="https://plotly.com/javascript/">plotly</a> code.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">plotly
</span><span class="sb">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "type": "scatter"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [16, 5, 11, 9],
      "type": "scatter"
    }
  ]
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>Which generates:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "type": "scatter"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [16, 5, 11, 9],
      "type": "scatter"
    }
  ]
}
</code></pre> <p>Also another example chart.</p> <div class="language-markdown highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="p">```</span><span class="nl">plotly
</span><span class="sb">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "mode": "markers"
    },
    {
      "x": [2, 3, 4, 5],
      "y": [16, 5, 11, 9],
      "mode": "lines"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [12, 9, 15, 12],
      "mode": "lines+markers"
    }
  ],
  "layout": {
    "title": {
      "text": "Line and Scatter Plot"
    }
  }
}</span>
<span class="p">```</span>
</code></pre></div></div> <p>This is how it looks like:</p> <pre><code class="language-plotly">{
  "data": [
    {
      "x": [1, 2, 3, 4],
      "y": [10, 15, 13, 17],
      "mode": "markers"
    },
    {
      "x": [2, 3, 4, 5],
      "y": [16, 5, 11, 9],
      "mode": "lines"
    },
    {
      "x": [1, 2, 3, 4],
      "y": [12, 9, 15, 12],
      "mode": "lines+markers"
    }
  ],
  "layout": {
    "title": {
      "text": "Line and Scatter Plot"
    }
  }
}
</code></pre>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="charts"/><summary type="html"><![CDATA[this is what included plotly.js code could look like]]></summary></entry><entry><title type="html">a post with image galleries</title><link href="https://codepydog.github.io/blog/2024/photo-gallery/" rel="alternate" type="text/html" title="a post with image galleries"/><published>2024-12-04T01:59:00+00:00</published><updated>2024-12-04T01:59:00+00:00</updated><id>https://codepydog.github.io/blog/2024/photo-gallery</id><content type="html" xml:base="https://codepydog.github.io/blog/2024/photo-gallery/"><![CDATA[<p>The images in this post are all zoomable, arranged into different mini-galleries using different libraries.</p> <h2 id="lightbox2"><a href="https://lokeshdhakar.com/projects/lightbox2/">Lightbox2</a></h2> <p><a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-lightbox="roadtrip"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p> <hr/> <h2 id="photoswipe"><a href="https://photoswipe.com/">PhotoSwipe</a></h2> <div class="pswp-gallery pswp-gallery--single-column" id="gallery--getting-started"> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg" data-pswp-width="1669" data-pswp-height="2500" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg" alt=""/> </a> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-2500.jpg" data-pswp-width="1875" data-pswp-height="2500" data-cropped="true" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/7/img-200.jpg" alt=""/> </a> <a href="https://unsplash.com" data-pswp-src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1666" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg" alt=""/> </a> <div> <a href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg" data-pswp-width="2500" data-pswp-height="1667" target="_blank"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg" alt=""/> </a> </div> </div> <hr/> <h2 id="spotlight-js"><a href="https://nextapps-de.github.io/spotlight/">Spotlight JS</a></h2> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/> </a> </div> <div class="spotlight-group"> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/4/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/5/img-200.jpg"/> </a> <a class="spotlight" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-2500.jpg"> <img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/6/img-200.jpg"/> </a> </div> <hr/> <h2 id="venobox"><a href="https://veno.es/venobox/">Venobox</a></h2> <p><a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/1/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/2/img-200.jpg"/></a> <a class="venobox" data-gall="myGallery" href="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-2500.jpg"><img src="https://cdn.photoswipe.com/photoswipe-demo-images/photos/3/img-200.jpg"/></a></p>]]></content><author><name></name></author><category term="sample-posts"/><category term="formatting"/><category term="images"/><summary type="html"><![CDATA[this is what included image galleries could look like]]></summary></entry><entry xml:lang="zh"><title type="html">[中文版] RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval</title><link href="https://codepydog.github.io/blog/2024/raptor-chs/" rel="alternate" type="text/html" title="[中文版] RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval"/><published>2024-09-18T10:00:00+00:00</published><updated>2024-09-18T10:00:00+00:00</updated><id>https://codepydog.github.io/blog/2024/raptor-chs</id><content type="html" xml:base="https://codepydog.github.io/blog/2024/raptor-chs/"><![CDATA[<div class="language-switcher" style="text-align: right; margin-bottom: 20px; padding: 8px 0; border-bottom: 1px solid #333;"> <span style="font-size: 14px; color: #888; margin-right: 8px;">🌐 Language:</span> <strong style="color: #007bff; font-weight: 600;">中文</strong> <span style="color: #666; margin: 0 6px;">|</span> <a href="/blog/2024/raptor-en/" style="color: #007bff; text-decoration: none; font-weight: 500; transition: opacity 0.2s;" onmouseover="this.style.opacity='0.7'" onmouseout="this.style.opacity='1'">English</a> </div> <blockquote> <p><strong>來源：</strong> <a href="https://arxiv.org/abs/2401.18059">Paper</a> / <a href="https://github.com/parthsarthi03/raptor">Official GitHub</a></p> </blockquote> <p>這是一篇來自 Stanford 的論文，全名為 <strong><a href="https://arxiv.org/abs/2401.18059">RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval</a></strong>。今年初剛出來時看到很多人在 Twitter 上分享，同時這篇也被 ICLR 接受，非常值得閱讀。</p> <p>相信讀者應該都具備基礎的 RAG 知識，這邊就不特別著墨了，直接來講論文的核心內容。</p> <h2 id="concept">Concept</h2> <h3 id="本文想解決的問題simple-rag-的缺陷">本文想解決的問題：Simple RAG 的缺陷</h3> <blockquote> <p>Despite a diversity in methods, the retrieving components of models predominantly rely on standard approaches, i.e., chunking corpora and encoding with BERT-based retrievers. Although this approach is widely adopted, Nair et al. (2023) highlights a potential shortcoming: contiguous segmentation might not capture the complete semantic depth of the text.</p> </blockquote> <p>RAG 的大致製作流程包含：將文本分塊 → 建立 embedding → query 進來時，尋找相關的文本當作 context。然而將文本分塊時，必然會產生上下文語意上的資訊流失，導致找不到相關的文本或資訊不完整。</p> <p>因此這篇論文想要解決的問題就是：<strong>如何在文本切塊的同時，盡可能地保留語意等級的資訊。</strong></p> <h3 id="隨著-llm-可接受的-context-變長為什麼還需要-rag">隨著 LLM 可接受的 context 變長，為什麼還需要 RAG？</h3> <blockquote> <p>Why Retrieval? … However, as Liu et al. (2023) and Sun et al. (2021) have noted, models tend to underutilize long-range context and see diminishing performance as context length increases, especially when pertinent information is embedded within a lengthy context.</p> </blockquote> <p>當今 LLM 發展趨勢的其中一項是：可以輸入 LLM 的 token 越來越長。例如：GPT-4o (128k), Claude 3.5 Sonnet (200k)。</p> <p>這時你可能會想，如果可以簡單地將所有 context 一次餵給 LLM，那還搞什麼 RAG，簡單地把所有東西塞給 LLM 就完事啦～</p> <p>當然實務上，如果只是要 Demo 搞一個 Baseline 版本，這樣的做法當然沒有問題。不過如果對模型輸出的質量有所追求的話，那麼必須知道 context 過長有以下的缺點：</p> <ol> <li><strong>較長的 context 模型越難以理解，輸出的質量較差。</strong></li> <li><strong>較長的 context 經常伴隨著無關的內容，這對模型理解造成負面影響。</strong></li> <li><strong>較長的 context 貴又慢。</strong></li> </ol> <p>基於上面這三點，目前 RAG 還是非常適合應用在 LLM 產品中！（暫時還不會被淘汰）</p> <h2 id="solution">Solution</h2> <h3 id="核心概念-idea">核心概念 (Idea)</h3> <p>前面提過，當前簡易的 RAG 系統最大的問題在於：分割文本時，容易丟失關鍵訊息，造成切塊的文本之間的語意不完整，導致檢索不夠精確。</p> <p>那麼怎麼保留丟失的關鍵訊息呢？</p> <p>本文提出的解法思路非常簡潔：<strong>透過不斷地群聚再總結，產生一個階層式的架構來盡可能保留語意相似的資訊。</strong></p> <div class="row"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/paper/RAPTOR_Recursive_Abstractive_Processing_for_Tree-Organized_Retrieval/raptor_figure1-480.webp 480w,/assets/img/paper/RAPTOR_Recursive_Abstractive_Processing_for_Tree-Organized_Retrieval/raptor_figure1-800.webp 800w,/assets/img/paper/RAPTOR_Recursive_Abstractive_Processing_for_Tree-Organized_Retrieval/raptor_figure1-1400.webp 1400w," type="image/webp" sizes="95vw"/> <img src="/assets/img/paper/RAPTOR_Recursive_Abstractive_Processing_for_Tree-Organized_Retrieval/raptor_figure1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" title="RAPTOR Tree Architecture" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <div class="caption"> Figure 1. RAPTOR Tree Architecture Overview. </div> <p>這篇論文的方法是將 leaf layer（最原始分塊後的文本）先做一次分群，接著把群裡文本分別進行一次總結，從而產生出新 layer 的文本（對應圖中 6,7,8）。不斷地循環這個過程，直到達到預設的 layer 上限，最終就可以建構出一個樹的結構，每一層 layer 都涵蓋前一層 layer 相關文本的完整資訊。</p> <p>簡單來說，整個流程大致如下：</p> <ol> <li><strong>先將原始的文本分塊，得到第一層 layer (leaf layer)。</strong> Ex: 一篇文章 → 5個 chunks (Figure 1. 中的 1,2,3,4,5)</li> <li><strong>對該層的文本進行分群。</strong> Ex: 5 chunks → 3個聚類 (Figure 1. 中的 layer 2 灰色區域)</li> <li><strong>對每個群的文本進行 summary，得到下一層的文本。</strong> Ex: 3 個聚類 → 3個新文本 (Figure 1. 中的 3,5 產生一個 summary 得到新層級的文本 6)</li> <li><strong>重複 2~3 步驟，直到達到 Max layer。</strong> Ex: Figure 1. 中最大的 layer 看起來是 3 層</li> </ol> <h3 id="分群-clustering">分群 (Clustering)</h3> <blockquote> <p>Clustering plays a key role in building the RAPTOR tree, organizing text segments into cohesive groups. This step groups related content together, which helps the subsequent retrieval process.</p> </blockquote> <p>RAPTOR 這篇論文最重要的就是分群。本質上，整篇論文就是透過遞迴分群法，產生一個階層狀的文本架構，讓 RAG 系統得到更豐沛的 context。</p> <p>在實作中，為了讓分群效果更好，作者做了一些特別處理：</p> <h4 id="soft-clustering">Soft Clustering</h4> <blockquote> <p>One of the unique aspects of our clustering approach is the use of soft clustering, where nodes can belong to multiple clusters without requiring a fixed number of clusters. This flexibility is essential because individual text segments often contain information relevant to various topics, thereby warranting their inclusion in multiple summaries.</p> </blockquote> <p>現實中，一個文本當然不只一種類型。因此作者考慮用軟分群 (soft clustering) 的分群演算法，就是 <strong>Gaussian Mixture Model (GMM)</strong>。</p> <p>如果對 GMM 不熟也沒關係，GMM 假設一個文本可能來自於多個群，只是屬於每個群的機率不同。</p> <h4 id="dimension-reduction--umap">Dimension Reduction — UMAP</h4> <blockquote> <p>The high dimensionality of vector embeddings presents a challenge for traditional GMMs, as distance metrics may behave poorly when used to measure similarity in high-dimensional spaces (Aggarwal et al., 2001). To mitigate this, we employ Uniform Manifold Approximation and Projection (UMAP), a manifold learning technique for dimensionality reduction (McInnes et al., 2018).</p> </blockquote> <p>作者指出，GMM 在高維度空間中效果不佳，因為高維度下距離計算的準確度下降，而分群的本質就是算距離。因此這邊作者打算將 embedding 後的文本做降維，具體使用的方法為 <strong>UMAP</strong>。（比方文本 embedding 後原本是 768 維度，用 UMAP 降到 20 維）</p> <p>那實務中，應該降到多少維度？</p> <p>雖然作者提到高維度分群效果不好來自於相關論文的結果。不過該論文沒提到具體應該要多少維度，但其實驗都是在維度 20 做的，可以參考這個數值看看。</p> <h4 id="global--local-clustering">Global &amp; Local Clustering</h4> <blockquote> <p>…it first identifies global clusters and then performs local clustering within these global clusters. This two-step clustering process captures a broad spectrum of relationships among the text data, from broad themes to specific details.</p> </blockquote> <p>作者設計兩階段的分群方法，先對一層 layer 先做一次 global clustering，接著對每個群裡面分別再分一次群。這樣不僅能捕捉高層級的群，也能兼顧每個群裡的細部項目。</p> <p>簡單來說，兩階段分群可以捕捉更多文本之間的關係。這邊大致瞭解為兩階段分群的用意就好，想知道細節可以去看實作。</p> <h4 id="bayesian-information-criterion-bic">Bayesian Information Criterion (BIC)</h4> <p>分群中最常見的問題就是群數要設多少？</p> <p>作者直接用 <strong>Bayesian Information Criterion</strong> 算資訊分數來決定要分多少群。下面是計算公式：</p> \[BIC = -2 \ln(L) + k \ln(N)\] <p>其中 N 表示文本片段的數量，k 表示模型參數的數量，而 L 是模型的最大概似函數值。</p> <h3 id="檢索-retrieve">檢索 (Retrieve)</h3> <p>前面的部分都是在講怎麼建立知識庫，接下來要講要怎麼檢索。</p> <p>共有兩種方法，各有優勢：</p> <h4 id="tree-traversal-遍歷樹">Tree Traversal (遍歷樹)</h4> <p>簡單來說，給定 root 的文本，將其所有 children 都找出來當作 retrieve 的相關文本。</p> <p><strong>優點：</strong> 資訊顆粒度均勻、拿到的文本包含廣泛和詳細的文本，資訊豐沛且多樣。</p> <p><strong>缺點：</strong> 檢索複雜、需要調整的地方比較多（如每一層要拿幾個？拿多深？）</p> <p>實作中，假設每一層都拿最相關的那個文本。</p> <h4 id="collapsed-tree-壓縮樹">Collapsed Tree (壓縮樹)</h4> <p>這個更簡單，直接將樹攤平，所有文本都是同一層的情況下去挑最相關的文本。</p> <p><strong>優點：</strong> 檢索簡單、適合回答特定問題（顆粒度恰好）</p> <p><strong>缺點：</strong> 檢索顆粒度粗細度不均、需要對每個文本都做檢索，計算消耗比較大。</p> <h4 id="實務上哪種檢索比較好">實務上哪種檢索比較好？</h4> <p>作者認為<strong>壓縮樹的表現平均來說優於遍歷樹</strong>。因為壓縮樹考慮所有文本的資訊相關性，得到的資訊顆粒度適合用來回答一些特定問題。反觀遍歷樹，不論怎麼設定每層的檢索數目，最後拿到的文本包含粗細的比例都保持不變。</p> <h2 id="總結">總結</h2> <ul> <li><strong>這篇論文解決什麼問題？</strong> 文本切割 → 語意不完整 → 檢索不準確</li> <li><strong>解法：</strong> 遞迴分群 → 產生多層次的文本，保留完整的語意</li> </ul> <hr/> <h2 id="references">References</h2> <ul> <li><a href="https://arxiv.org/abs/2307.03172">Lost in the Middle: How Language Models Use Long Contexts</a></li> <li><a href="https://arxiv.org/abs/2409.01666">In Defense of RAG in the Era of Long-Context Language Models</a></li> <li><a href="https://link.springer.com/chapter/10.1007/3-540-44503-X_27">On the Surprising Behavior of Distance Metrics in High Dimensional Space</a></li> <li><a href="https://arxiv.org/abs/2409.04701">Late Chunking: Balancing Precision and Cost in Long Context Retrieval</a></li> </ul>]]></content><author><name></name></author><category term="paper"/><category term="chinese"/><category term="rag"/><category term="retrieval"/><category term="clustering"/><summary type="html"><![CDATA[Stanford 提出的 RAPTOR 方法透過遞迴分群和總結建立階層式文本架構，有效解決傳統 RAG 系統在文本切塊時的語意資訊流失問題。該方法使用軟分群、UMAP 降維和貝葉斯資訊準則來優化檢索效果，在保留完整語意的同時提升檢索準確性。]]></summary></entry></feed>